{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":60892,"databundleVersionId":6989718,"sourceType":"competition"},{"sourceId":6612067,"sourceType":"datasetVersion","datasetId":3815527}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><img src=\"https://raw.githubusercontent.com/mateuszk098/kaggle_notebooks/master/playground_series_s3e25/undraw_Developer_activity_re_39tg.png\" width=400px></center>\n\n# <p style=\"font-family: 'JetBrains Mono'; font-weight: bold; font-size: 125%; color: #4A4B52; text-align: center\">Playground Series S3E25 - Mohs Hardness</p>","metadata":{}},{"cell_type":"code","source":"##https://www.kaggle.com/code/mateuszk013/playground-series-s3e25-mohs-hardness\n##https://www.kaggle.com/mateuszk013\n\n# %load ../utils/config.py\n!pip install -q kaleido\nimport glob\nimport operator\nimport os\nimport shutil\nimport subprocess\nimport sys\nimport warnings\nfrom array import array\nfrom collections import defaultdict, namedtuple\nfrom copy import copy\nfrom functools import partial, singledispatch\nfrom itertools import chain, combinations, product\nfrom pathlib import Path\nfrom time import strftime\n\nimport joblib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optuna\nimport pandas as pd\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nimport plotly.io as pio\nimport scipy.stats as stats\nimport seaborn as sns\nimport shap\nfrom colorama import Fore, Style\nfrom IPython.display import HTML, Image, display_html\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom plotly.subplots import make_subplots\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.spatial.distance import squareform\nfrom sklearn import clone\nfrom sklearn.base import (\n    BaseEstimator,\n    ClassNamePrefixFeaturesOutMixin,\n    MetaEstimatorMixin,\n    OneToOneFeatureMixin,\n    TransformerMixin,\n)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cluster import FeatureAgglomeration\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.discriminant_analysis import StandardScaler\nfrom sklearn.ensemble import (\n    GradientBoostingRegressor,\n    IsolationForest,\n    RandomForestRegressor,\n)\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom sklearn.linear_model import LogisticRegression, SGDOneClassSVM\nfrom sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding\nfrom sklearn.metrics import (\n    confusion_matrix,\n    median_absolute_error,\n    roc_auc_score,\n    roc_curve,\n)\nfrom sklearn.model_selection import (\n    KFold,\n    StratifiedKFold,\n    cross_val_predict,\n    cross_val_score,\n)\nfrom sklearn.neighbors import KNeighborsRegressor, LocalOutlierFactor\nfrom sklearn.pipeline import FunctionTransformer, make_pipeline, make_union\nfrom sklearn.preprocessing import MinMaxScaler, PowerTransformer, RobustScaler\nfrom sklearn.svm import SVC, SVR, LinearSVR\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.utils.validation import check_array, check_is_fitted\nfrom xgboost import XGBClassifier\n\n# Environment\nON_KAGGLE = os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") is not None\n\n# Colorama settings.\nCLR = (Style.BRIGHT + Fore.BLACK) if ON_KAGGLE else (Style.BRIGHT + Fore.WHITE)\nRED = Style.BRIGHT + Fore.RED\nBLUE = Style.BRIGHT + Fore.BLUE\nCYAN = Style.BRIGHT + Fore.CYAN\nMAGENTA = Style.BRIGHT + Fore.MAGENTA\nRESET = Style.RESET_ALL\n\n# Data Frame and Plotly colors.\nFONT_COLOR = \"#4A4B52\"\nBACKGROUND_COLOR = \"#FFFCFA\"\nGRADIENT_COLOR = \"#BAB8B8\"\n# Define as numpy array because it supports fancy indexing.\nCOLOR_SCHEME = np.array((\"#4A4B52\", \"#FCFCFC\", \"#E8BA91\"))\n# Ticks size for plotly and matplotlib.\nTICKSIZE = 11\n\n# Set Plotly theme.\npio.templates[\"minimalist\"] = go.layout.Template(\n    layout=go.Layout(\n        font_family=\"Open Sans\",\n        font_color=FONT_COLOR,\n        title_font_size=20,\n        plot_bgcolor=BACKGROUND_COLOR,\n        paper_bgcolor=BACKGROUND_COLOR,\n        xaxis=dict(tickfont_size=TICKSIZE, titlefont_size=TICKSIZE, showgrid=False),\n        yaxis=dict(tickfont_size=TICKSIZE, titlefont_size=TICKSIZE, showgrid=False),\n        width=840,\n        height=540,\n        legend=dict(yanchor=\"bottom\", xanchor=\"right\", orientation=\"h\", title=\"\"),\n    ),\n    layout_colorway=COLOR_SCHEME,\n)\npio.templates.default = \"plotly+minimalist\"\n\nMATPLOTLIB_THEME = {\n    \"axes.labelcolor\": FONT_COLOR,\n    \"axes.labelsize\": TICKSIZE,\n    \"axes.facecolor\": BACKGROUND_COLOR,\n    \"axes.titlesize\": 14,\n    \"axes.grid\": False,\n    \"xtick.labelsize\": TICKSIZE,\n    \"xtick.color\": FONT_COLOR,\n    \"ytick.labelsize\": TICKSIZE,\n    \"ytick.color\": FONT_COLOR,\n    \"figure.facecolor\": BACKGROUND_COLOR,\n    \"figure.edgecolor\": BACKGROUND_COLOR,\n    \"figure.titlesize\": 14,\n    \"figure.dpi\": 72,  # Locally Seaborn uses 72, meanwhile Kaggle 96.\n    \"text.color\": FONT_COLOR,\n    \"font.size\": TICKSIZE,\n    \"font.family\": \"Serif\",\n}\nsns.set_theme(rc=MATPLOTLIB_THEME)\n\n# Define Data Frame theme.\nCELL_HOVER = {  # for row hover use <tr> instead of <td>\n    \"selector\": \"td:hover\",\n    \"props\": f\"background-color: {BACKGROUND_COLOR}\",\n}\nTEXT_HIGHLIGHT = {\n    \"selector\": \"td\",\n    \"props\": f\"color: {FONT_COLOR}; font-weight: bold\",\n}\nINDEX_NAMES = {\n    \"selector\": \".index_name\",\n    \"props\": f\"font-weight: normal; background-color: {BACKGROUND_COLOR}; color: {FONT_COLOR};\",\n}\nHEADERS = {\n    \"selector\": \"th:not(.index_name)\",\n    \"props\": f\"font-weight: normal; background-color: {BACKGROUND_COLOR}; color: {FONT_COLOR};\",\n}\nDF_STYLE = (INDEX_NAMES, HEADERS, TEXT_HIGHLIGHT)\nDF_CMAP = sns.light_palette(GRADIENT_COLOR, as_cmap=True)\n\n# Html style for table of contents, code highlight and url.\nHTML_STYLE = \"\"\"\n    <style>\n    code {\n        background: rgba(42, 53, 125, 0.10) !important;\n        border-radius: 4px !important;\n    }\n    a {\n        color: rgba(123, 171, 237, 1.0) !important;\n    }\n    ol.numbered-list {\n        counter-reset: item;\n    }\n    ol.numbered-list li {\n        display: block;\n    }\n    ol.numbered-list li:before {\n        content: counters(item, '.') '. ';\n        counter-increment: item;\n    }\n    </style>\n\"\"\"\n\n\n# Utility functions.\ndef download_from_kaggle(expr, /, data_dir=None):\n    \"\"\"Download all files from the Kaggle competition/dataset.\n\n    Args:\n        expr: Match expression to be used by kaggle API, e.g.\n            \"kaggle competitions download -c competition\" or\n            \"kaggle datasets download -d user/dataset\".\n        data_dir: Optional. Directory path where to save files. Default to `None`,\n        which means that files will be downloaded to `data` directory.\n\n    Notes:\n        If the associated files already exists, then it does nothing.\n    \"\"\"\n\n    if data_dir is None:\n        data_dir = Path(\"data/\")\n    else:\n        data_dir = Path(data_dir)\n\n    match expr.split():\n        case [\"kaggle\", _, \"download\", *args] if args:\n            data_dir.mkdir(parents=True, exist_ok=True)\n            filename = args[-1].split(\"/\")[-1] + \".zip\"\n            if not (data_dir / filename).is_file():\n                subprocess.run(expr)\n                shutil.unpack_archive(filename, data_dir)\n                shutil.move(filename, data_dir)\n        case _:\n            raise SyntaxError(\"Invalid expression!\")\n\n\ndef get_interpolated_colors(color1, color2, /, n_colors=1):\n    \"\"\"Return `n_colors` colors in HEX format, interpolated beetwen `color1` and `color2`.\n\n    Args:\n        color1: Initial HEX color to be interpolated from.\n        color2: Final HEX color to be interpolated from.\n        n_colors: Optional. Number of colors to be interpolated between `color1`\n            and `color2`. Default to 1.\n\n    Returns:\n        colors: List of colors interpolated between `color1` and `color2`.\n    \"\"\"\n\n    def interpolate(color1, color2, t):\n        r1, g1, b1 = int(color1[1:3], 16), int(color1[3:5], 16), int(color1[5:7], 16)\n        r2, g2, b2 = int(color2[1:3], 16), int(color2[3:5], 16), int(color2[5:7], 16)\n        r = int(r1 + (r2 - r1) * t)\n        g = int(g1 + (g2 - g1) * t)\n        b = int(b1 + (b2 - b1) * t)\n        return f\"#{r:02X}{g:02X}{b:02X}\"\n\n    return [interpolate(color1, color2, k / (n_colors + 1)) for k in range(1, n_colors + 1)]\n\n\ndef get_pretty_frame(frame, /, gradient=False, formatter=None, precision=3, repr_html=False):\n    stylish_frame = frame.style.set_table_styles(DF_STYLE).format(\n        formatter=formatter, precision=precision\n    )\n    if gradient:\n        stylish_frame = stylish_frame.background_gradient(DF_CMAP)  # type: ignore\n    if repr_html:\n        stylish_frame = stylish_frame.set_table_attributes(\"style='display:inline'\")._repr_html_()\n    return stylish_frame\n\n\ndef numeric_descr(frame, /):\n    return (\n        frame.describe(percentiles=(0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99))\n        .T.drop(\"count\", axis=1)\n        .rename(columns=str.title)\n    )\n\n\ndef frame_summary(frame, /):\n    missing_vals = frame.isna().sum()\n    missing_vals_ratio = missing_vals / len(frame)\n    unique_vals = frame.apply(lambda col: len(col.unique()))\n    most_freq_count = frame.apply(lambda col: col.value_counts().iloc[0])\n    most_freq_val = frame.mode().iloc[:1].T.squeeze()\n    unique_ratio = unique_vals / len(frame)\n    freq_count_ratio = most_freq_count / len(frame)\n\n    return pd.DataFrame(\n        {\n            \"Dtype\": frame.dtypes,\n            \"MissingValues\": missing_vals,\n            \"MissingValuesRatio\": missing_vals_ratio,\n            \"UniqueValues\": unique_vals,\n            \"UniqueValuesRatio\": unique_ratio,\n            \"MostFreqValue\": most_freq_val,\n            \"MostFreqValueCount\": most_freq_count,\n            \"MostFreqValueCountRatio\": freq_count_ratio,\n        }\n    )\n\n\ndef check_categories_alignment(frame1, frame2, /, out_color=BLUE):\n    print(CLR + \"The same categories in training and test datasets?\\n\")\n    cat_features = frame2.select_dtypes(include=\"object\").columns.to_list()\n\n    for feature in cat_features:\n        frame1_unique = set(frame1[feature].unique())\n        frame2_unique = set(frame2[feature].unique())\n        same = np.all(frame1_unique == frame2_unique)\n        print(CLR + f\"{feature:25s}\", out_color + f\"{same}\")\n\n\ndef get_lower_triangular_frame(frame, /):\n    if not frame.shape[0] == frame.shape[1]:\n        raise ValueError(f\"{type(frame)!r} is not square frame\")\n    lower_triu = np.triu(np.ones_like(frame, dtype=bool))\n    frame = frame.mask(lower_triu)\n    return frame.dropna(axis=\"index\", how=\"all\").dropna(axis=\"columns\", how=\"all\")\n\n\ndef save_and_show_fig(fig, filename, /, img_dir=None, format=\"png\"):\n    if img_dir is None:\n        img_dir = Path(\"images\")\n    if not isinstance(img_dir, Path):\n        raise TypeError(\"The `img_dir` argument must be `Path` instance!\")\n\n    img_dir.mkdir(parents=True, exist_ok=True)\n    fig_path = img_dir / (filename + \".\" + format)\n    fig.write_image(fig_path)\n\n    return Image(fig.to_image(format=format))\n\n\ndef get_n_rows_and_axes(n_features, n_cols, /, start_at=1):\n    n_rows = int(np.ceil(n_features / n_cols))\n    current_col = range(start_at, n_cols + start_at)\n    current_row = range(start_at, n_rows + start_at)\n    return n_rows, tuple(product(current_row, current_col))\n\n\ndef get_kde_estimation(\n    series,\n    *,\n    bw_method=None,\n    weights=None,\n    percentile_range=(0, 100),\n    estimate_points_frac=0.1,\n    space_extension_frac=0.01,\n    cumulative=False,\n):\n    \"\"\"Return pdf dictionary for set of points using gaussian kernel density estimation.\n\n    Args:\n        series: The dataset with which `stats.gaussian_kde` is initialized.\n        bw_method: Optional. The method used to calculate the estimator bandwidth.\n        This can be 'scott', 'silverman', a scalar constant or a callable. If a scalar,\n        this will be used directly as `kde.factor`. If a callable, it should take\n        a `stats.gaussian_kde` instance as only parameter and return a scalar.\n        If `None` (default), 'scott' is used.\n        weights: Optional. Weights of datapoints. This must be the same shape as dataset.\n        If `None` (default), the samples are assumed to be equally weighted.\n        percentile_range: Optional. Percentile range of the `series` to create estimated space.\n        By default (0, 100) range is used.\n        estimate_points_frac: Optional. Fraction of `series` length to create linspace for\n        estimated points.\n        space_extension_frac: Optional. Estimation space will be extended by\n        `space_extension_frac * len(series)` for both edges.\n        cumulative: Optional. Whether to calculate cdf. Default to `False`.\n\n    Returns:\n        Dictionary with kde space, values, and cumulative values if `cumulative` is `True`.\n    \"\"\"\n\n    series = pd.Series(series).dropna()\n    kde = stats.gaussian_kde(series, bw_method=bw_method, weights=weights)\n    start, stop = np.percentile(series, percentile_range)\n\n    n_points = int(estimate_points_frac * len(series))\n    n_extend = int(space_extension_frac * len(series))\n\n    if n_extend > 0:\n        dx = (stop - start) / (n_points - 1)\n        start, stop = start - n_extend * dx, stop + n_extend * dx\n\n    kde_space = np.linspace(start, stop, n_points)\n    kde_vals = kde.evaluate(kde_space)\n    results = {\"space\": kde_space, \"vals\": kde_vals}\n\n    if cumulative:\n        kde_vals_cum = np.cumsum(kde_vals)\n        return results | {\"vals_cumulative\": kde_vals_cum / kde_vals_cum.max()}\n\n    return results\n\n\ndef unit_norm(x):\n    return x / np.sum(x)\n\n\n# Html highlight. Must be included at the end of all imports!\nHTML(HTML_STYLE)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n\">\n    <b>Competition Description</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    The dataset for this competition (both train and test) was generated from a deep learning model trained on the <a href=\"https://www.kaggle.com/datasets/jocelyndumlao/prediction-of-mohs-hardness-with-machine-learning\"><b>Prediction of Mohs Hardness with Machine Learning Dataset</b></a>. Feature distributions are close to, but not exactly the same, as the original. Full description of the original dataset is as follows.:<br><br>\n    <i>Hardness, or the quantitative value of resistance to permanent or plastic deformation, plays a very crucial role in materials design in many applications, such as ceramic coatings and abrasives. Hardness testing is an especially useful method as it is non-destructive and simple to implement to gauge the plastic properties of a material. In this study, I proposed a machine, or statistical, learning approach to predict hardness in naturally occurring materials, which integrates atomic and electronic features from composition directly across a wide variety of mineral compositions and crystal systems. First, atomic and electronic features from the composition, such as van der Waals and covalent radii as well as the number of valence electrons, were extracted from the composition.<br><br>\n    In this study, the author trained a set of classifiers to understand whether compositional features can be used to predict the Mohs hardness of minerals of different chemical spaces, crystal structures, and crystal classes. The dataset for training and testing the classification models used in this study originated from experimental Mohs hardness data, their crystal classes, and chemical compositions of naturally occurring minerals reported in the Physical and Optical Properties of Minerals CRC Handbook of Chemistry and Physics and the American Mineralogist Crystal Structure Database. The database is composed of 369 uniquely named minerals. Due to the presence of multiple composition combinations for minerals referred to by the same name, the first step was to perform compositional permutations on these minerals. This produced a database of 622 minerals of unique compositions, comprising 210 monoclinic, 96 rhombohedral, 89 hexagonal, 80 tetragonal, 73 cubic, 50 orthorhombic, 22 triclinic, 1 trigonal, and 1 amorphous structure. An independent dataset was compiled to validate the model performance. The validation dataset contains the composition, crystal structure, and Mohs hardness values of 51 synthetic single crystals reported in the literature. The validation dataset includes 15 monoclinic, 7 tetragonal, 7 hexagonal, 6 orthorhombic, 4 cubic, and 3 rhombohedral crystal structures.<br><br>\n    In this study, the author constructed a database of compositional feature descriptors that characterize naturally occurring materials obtained directly from the Physical and Optical Properties of Minerals CRC Handbook45. This comprehensive compositional-based dataset allows us to train models that are able to predict hardness across a wide variety of mineral compositions and crystal classes. Each material in both the naturally occurring mineral and artificial single crystal datasets was represented by 11 atomic descriptors. The elemental features are the number of electrons, number of valence electrons, atomic number, Pauling electronegativity of the most common oxidation state, covalent atomic radii, van der Waals radii, and ionization energy of neutral.</i><br><br>\n    <b>More in the paper: <a href=\"https://par.nsf.gov/servlets/purl/10187152\">Prediction of Mohs Hardness with Machine Learning Methods Using Compositional Features</a></b>\n</p>\n    \n<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n\">\n    <b>Task</b> 💡\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    This is a regression problem, where the main task is to predict the <code>Hardness</code> feature. The competition evaluation metric is <a href=\"https://en.wikipedia.org/wiki/Median_absolute_deviation\"><b>Median Absolute Error (MedAE)</b></a>:\n    \\[\\textrm{MedAE} =\\textrm{median}\\left(|y_i - \\hat{y}_i|, \\ldots, |y_n - \\hat{y}_n|\\right),\\]\n    where $\\hat{y}_i$ is the predicted value and $y_i$ is the ground truth for each observation.\n</p>\n\n<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n\">\n    <b>Table of Contents</b> 📔\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    The table of contents provides pleasurable navigation through the whole notebook. You can easily navigate through sections and return to TOC. If you want quickly find out something about the dataset, just read the first section, i.e. <b>Quick Overview</b>.\n</p>\n\n<blockquote class=\"anchor\" id=\"top\" style=\"\n    margin-right: auto; \n    margin-left: auto;\n    padding: 10px;\n    background-color: #E8BA91;\n    border-radius: 2px;\n    border: 1px solid #E8BA91;\n\">\n<ol class=\"numbered-list\" style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    color: #F2F2F0;\n    margin-top: 15px;\n    margin-bottom: 15px;\n\">\n    <li><a href=\"#quick_overview\"><span style=\"color: #F2F2F0\">Quick Overview</span></a>\n        <ol class=\"numbered-list\" class=\"numbered-list\" style=\"\n            font-size: 16px;\n            font-family: 'JetBrains Mono';\n        \">\n            <li><a href=\"#data_reading_and_features_description\"><span style=\"color: #F2F2F0\">Data Reading &amp; Features Description</span></a></li>\n            <li><a href=\"#basic_numerical_properties_summaries\"><span style=\"color: #F2F2F0\">Basic Numerical Properties &amp; Summaries</span></a></li>\n            <li><a href=\"#probability_plots_and_example_transformations\"><span style=\"color: #F2F2F0\">Probability Plots &amp; Example Transformations</span></a></li>\n        </ol>\n    </li>\n    <li><a href=\"#features_importance\"><span style=\"color: #F2F2F0\">Features Importance</span></a>\n        <ol class=\"numbered-list\" class=\"numbered-list\" style=\"\n            font-size: 16px;\n            font-family: 'JetBrains Mono';\n        \">\n            <li><a href=\"#simple_decision_tree_and_its_decision_process\"><span style=\"color: #F2F2F0\">Decision Process in Simple Decision Tree</span></a></li>\n            <li><a href=\"#random_variables_permutation_test\"><span style=\"color: #F2F2F0\">Random Variables &amp; Permutation Test</span></a></li>\n            <li><a href=\"#mutual_information\"><span style=\"color: #F2F2F0\">Mutual Information</span></a></li>\n            <li><a href=\"#partial_dependence_for_features_of_interest\"><span style=\"color: #F2F2F0\">Partial Dependence for Features of Interest</span></a></li>\n        </ol>\n    </li>\n    <li><a href=\"#dimensionality_reduction\"><span style=\"color: #F2F2F0\">Dimensionality Reduction</span></a></li>\n    <li><a href=\"#outliers_detection\"><span style=\"color: #F2F2F0\">Outliers Detection</span></a></li>\n    <li><a href=\"#modelling\"><span style=\"color: #F2F2F0\">Modelling</span></a>\n        <ol class=\"numbered-list\" class=\"numbered-list\" style=\"\n            font-size: 16px;\n            font-family: 'JetBrains Mono';\n        \">\n            <li><a href=\"#regression_approach\"><span style=\"color: #F2F2F0\">Regression Approach</span></a></li>\n            <li><a href=\"#multiclass_classification_approach\"><span style=\"color: #F2F2F0\">Multiclass Classification Approach</span></a></li>\n        </ol>\n    </li>\n    <li><a href=\"#summary\"><span style=\"color: #F2F2F0\">Summary</span></a></li>\n</ol>\n</blockquote>","metadata":{}},{"cell_type":"markdown","source":"# <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">1</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Quick Overview</span></b><a class=\"anchor\" id=\"quick_overview\"></a> [↑](#top)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>About Section</b> 💡\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    In this section, I provide a quick overview of the dataset. More detailed analysis will be done in subsequent sections.\n</p>","metadata":{}},{"cell_type":"markdown","source":"## <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">1.1</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Data Reading &amp; Features Description</span></b><a class=\"anchor\" id=\"data_reading_and_features_description\"></a> [↑](#top)","metadata":{}},{"cell_type":"code","source":"competition = \"playground-series-s3e25\"\nexpr = f\"kaggle competitions download -c {competition}\"\n\nif not ON_KAGGLE:\n    download_from_kaggle(expr)\n    train_path = \"data/train.csv\"\n    test_path = \"data/test.csv\"\nelse:\n    train_path = f\"/kaggle/input/{competition}/train.csv\"\n    test_path = f\"/kaggle/input/{competition}/test.csv\"\n\ntrain = pd.read_csv(train_path, index_col=\"id\")  # .rename(columns=str.title)\ntest = pd.read_csv(test_path, index_col=\"id\")  # .rename(columns=str.title)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_pretty_frame(train.head())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info(verbose=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info(verbose=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"  \n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n\">\n  <b>Features Description</b> 📔\n</p>\n\n<center>\n<table style=\"\n  padding: 10px;\n  border-radius: 2px;\n  border: 4px solid #4A4B52;\n  font-family: 'JetBrains Mono';\n  font-size: 16px;\n  width: 100%\";\n>\n  <tr>\n    <th>Feature</th>\n    <th>Description</th>\n  </tr>\n  <tr>\n    <td>allelectrons_Total</td>\n    <td>Total number of electrons.</td>\n  </tr>\n  <tr>\n    <td>density_Total</td>\n    <td>Total elemental density.</td>\n  </tr>\n  <tr>\n    <td>allelectrons_Average</td>\n    <td>Atomic average number of electrons.</td>\n  </tr>\n  <tr>\n    <td>val_e_Average</td>\n    <td>Atomic average number of valence electrons.</td>\n  </tr>\n  <tr>\n    <td>atomicweight_Average</td>\n    <td>Atomic average atomic weight.</td>\n  </tr>\n  <tr>\n    <td>ionenergy_Average</td>\n    <td>Atomic average frst IE (ionization energy).</td>\n  </tr>\n  <tr>\n    <td>el_neg_chi_Average</td>\n    <td>Atomic average Pauling electronegativity of the most common oxidation state.</td>\n  </tr>\n  <tr>\n    <td>R_vdw_element_Average</td>\n    <td>Atomic average van der Waals atomic radius.</td>\n  </tr>\n  <tr>\n    <td>R_cov_element_Average</td>\n    <td>Atomic average covalent atomic radius.</td>\n  </tr>\n  <tr>\n    <td>zaratio_Average</td>\n    <td>Atomic average atomic number to mass number ratio.</td>\n  </tr>\n  <tr>\n    <td>density_Average</td>\n    <td>Atomic average elemental density.</td>\n  </tr>\n  <tr>\n    <td>Hardness</td>\n    <td>A mineral's hardness is a measure of its relative resistance to scratching, measured by scratching the mineral against another substance of known hardness on the Mohs Hardness Scale.</td>\n  </tr>\n</table>\n</center>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    So, here we've got $11$ continuous features and a continuous target. Moreover, there are $10407$ samples in the training data and $6939$ in the test dataset. This gives $946$ entries per dimension during training, so there should be enough data to learn patterns. Let's get to the basic numerical summaries.\n</p>","metadata":{}},{"cell_type":"markdown","source":"## <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">1.2</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Basic Numerical Properties &amp; Summaries</span></b><a class=\"anchor\" id=\"basic_numerical_properties_summaries\"></a> [↑](#top)","metadata":{}},{"cell_type":"code","source":"fig = px.histogram(\n    train,\n    x=\"Hardness\",\n    histnorm=\"probability\",\n    marginal=\"box\",\n    height=460,\n    title=\"Distribution of Hardness - Target Variable<br>\"\n    \"<span style='font-size: 75%; font-weight: bold;'>\"\n    \"This feature seems like a quantized one (many repetitive values)</span>\",\n)\nfig.update_yaxes(title=\"Probability\", row=1)\nsave_and_show_fig(fig, \"hardness_distribution\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(CLR + \"Training Dataset:\")\ntrain_summary = frame_summary(train)\nget_pretty_frame(train_summary, gradient=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(CLR + \"Test Dataset:\")\ntest_summary = frame_summary(test)\nget_pretty_frame(test_summary, gradient=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Missing &amp; Unique &amp; Most Frequent Values</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    What's surprising there is no missing values, so we don't need to bother about imputation. We can see the ratio of unique values in both datasets is quite low for all features, which means that features consist of many repeatable values. This makes them more semi-continuous rather than continuous. The target variable has only $50$ unique values among more than $10000$ training samples, and in my opinion, it's more like a categorical feature.\n</p>","metadata":{}},{"cell_type":"code","source":"print(CLR + \"Training Dataset:\")\ntrain_num_descr = numeric_descr(train)\nget_pretty_frame(train_num_descr, gradient=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(CLR + \"Test Dataset:\")\ntest_num_descr = numeric_descr(test)\nget_pretty_frame(test_num_descr, gradient=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Numerical Summary</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Here we have two features with some outliers. These are <code>allelectrons_Total</code> and <code>density_Total</code>. Nevertheless, both datasets overlap (see respective percentiles in both cases) for all attributes. It is most probable that they come from the same distribution.<br><br>\n    Let's test the last consideration with <b>adversarial validation</b>. What is the adversarial validation? <b>Well, it's a very straightforward way to check whether our subsets are similar (sampled from the same or very similar distributions).</b> We label training and test datasets with, for example, $0$ and $1$. Then, we combine them into one dataset and shuffle them. Subsequently, we can perform binary classification and assess if we're able to identify which observation is from which dataset. When we get a ROC value of around $0.5$ (random guessing), they are indistinguishable, and this case is desired. On the other hand, when ROC $>0.5$, it probably means that training and test subsets are from different distributions.\n</p>","metadata":{}},{"cell_type":"code","source":"train_av = train.drop(\"Hardness\", axis=1).assign(AV=0)\ntest_av = test.assign(AV=1)\n\ndata_av = pd.concat((train_av, test_av), ignore_index=True)\ndata_av = data_av.sample(frac=1.0, random_state=42)\n\nX = data_av.drop(\"AV\", axis=1)\ny = data_av.AV\n\ny_proba = cross_val_predict(\n    estimator=make_pipeline(StandardScaler(), LogisticRegression(random_state=42)),\n    X=X,\n    y=y,\n    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=19937),\n    method=\"predict_proba\",\n)\n\nav_scores = {\n    \"ConfusionMatrix\": confusion_matrix(y, y_proba.argmax(axis=1)),\n    \"FPR-TPR-Threshold\": roc_curve(y, y_proba[:, 1]),\n    \"ROC-AUC\": roc_auc_score(y, y_proba[:, 1]),\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_scatter(\n    x=av_scores[\"FPR-TPR-Threshold\"][0],\n    y=av_scores[\"FPR-TPR-Threshold\"][1],\n    name=\"AV Result\",\n    mode=\"lines\",\n    line_color=COLOR_SCHEME[2],\n)\nfig.add_scatter(\n    x=[0, 1],\n    y=[0, 1],\n    name=\"Random Guess\",\n    mode=\"lines\",\n    line=dict(dash=\"longdash\", color=COLOR_SCHEME[0]),\n)\nfig.add_annotation(\n    x=0.05,\n    y=0.85,\n    align=\"left\",\n    xanchor=\"left\",\n    text=f\"<b>AV ROC-AUC: {av_scores['ROC-AUC']:.5f}<br>\" \"Conclusion? The same distribution.\",\n    showarrow=False,\n    font_size=14,\n)\nfig.update_yaxes(\n    scaleanchor=\"x\",\n    scaleratio=1,\n    range=(-0.01, 1.01),\n    title=\"True Positive Rate (Recall)\",\n)\nfig.update_xaxes(\n    scaleanchor=\"y\",\n    scaleratio=1,\n    range=(-0.01, 1.01),\n    title=\"False Positive Rate (Fall-Out)\",\n)\nfig.update_layout(\n    title=\"Adversarial Validation Results<br>\"\n    \"<span style='font-size: 75%; font-weight: bold;'>\"\n    \"Training and test datasets are indistinguishable</span>\",\n    width=540,\n    legend=dict(y=1.0, x=1.2),\n)\nsave_and_show_fig(fig, \"adversarial_validation\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Adversarial Validation Results</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    So, the result is excellent for us since ROC $\\approx 0.5$ means that subsets are indistinguishable (they come from the same distribution). Let's see histograms yet.\n</p>","metadata":{}},{"cell_type":"code","source":"features = test.columns.to_list()\n\nn_cols = 3\nn_rows, axes = get_n_rows_and_axes(len(features), n_cols)\n\nfig = make_subplots(\n    rows=n_rows,\n    cols=n_cols,\n    y_title=\"Probability Density\",\n    horizontal_spacing=0.1,\n    vertical_spacing=0.1,\n).update_annotations(font_size=14)\n\nfor frame, color, group in zip((train, test), (COLOR_SCHEME[0], COLOR_SCHEME[2]), (\"Train\", \"Test\")):\n    for k, (var, (row, col)) in enumerate(zip(features, axes), start=1):\n        start, end = np.percentile(frame[var], (1, 99))\n        fig.add_histogram(\n            x=frame[var],\n            xbins=go.histogram.XBins(start=start, end=end),\n            histnorm=\"probability density\",\n            marker_color=color,\n            marker_line_width=0,\n            opacity=0.8,\n            name=group,\n            legendgroup=group,\n            showlegend=k == 1,\n            row=row,\n            col=col,\n        )\n        fig.update_xaxes(title_text=f\"<b>{var}</b>\", row=row, col=col)\n\nfig.update_layout(\n    width=840,\n    height=740,\n    legend=dict(y=1, x=1),\n    title=\"Training & Test Feature Histograms<br>\"\n    \"<span style='font-size: 75%; font-weight: bold;'>\"\n    \"Restricted to (1, 99) percentile range to avoid showing extreme outliers</span>\",\n    bargap=0,\n    bargroupgap=0,\n)\nsave_and_show_fig(fig, \"histograms\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_cols = 3\nn_rows, axes = get_n_rows_and_axes(len(features), n_cols)\n\nfig = make_subplots(\n    rows=n_rows,\n    cols=n_cols,\n    y_title=\"Probability Density\",\n    horizontal_spacing=0.1,\n    vertical_spacing=0.1,\n).update_annotations(font_size=14)\n\nfor frame, color, group in zip((train, test), (COLOR_SCHEME[0], COLOR_SCHEME[2]), (\"Train\", \"Test\")):\n    for k, (var, (row, col)) in enumerate(zip(features, axes), start=1):\n        kde = get_kde_estimation(frame[var], percentile_range=(1, 99))\n        fig.add_scatter(\n            x=kde[\"space\"],\n            y=kde[\"vals\"],\n            line=dict(dash=\"solid\", color=color, width=1),\n            fill=\"tozeroy\",\n            name=group,\n            legendgroup=group,\n            showlegend=k == 1,\n            row=row,\n            col=col,\n        )\n        fig.update_xaxes(title_text=f\"<b>{var}</b>\", row=row, col=col)\n\nfig.update_layout(\n    width=840,\n    height=740,\n    legend=dict(y=1, x=1),\n    title=\"Training & Test Feature KDEs<br>\"\n    \"<span style='font-size: 75%; font-weight: bold;'>\"\n    \"Restricted to (1, 99) percentile range to avoid showing extreme outliers</span>\",\n)\nsave_and_show_fig(fig, \"kdes\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Feature Distributions</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Feature distributions confirm the previous statement, i.e. training and test subsets probably follow the same distribution, which is reflected as bins overlapping and similar density estimation. Let's continue analysis with correlation matrix and hierarchical clustering. \n</p>","metadata":{}},{"cell_type":"code","source":"pearson_corr = train.corr(method=\"pearson\")\nlower_triu_corr = get_lower_triangular_frame(pearson_corr)\ncolormap = tuple(zip((0, 0.5, 1), COLOR_SCHEME[[1, 0, 2]]))\n\nheatmap = go.Heatmap(\n    z=lower_triu_corr,\n    x=lower_triu_corr.columns,\n    y=lower_triu_corr.index,\n    text=lower_triu_corr.fillna(\"\"),\n    texttemplate=\"%{text:.2f}\",\n    xgap=4,\n    ygap=4,\n    showscale=True,\n    colorscale=colormap,\n    colorbar_len=1.02,\n    hoverinfo=\"none\",\n)\nfig = go.Figure(heatmap)\nfig.update_layout(\n    title=\"Training Dataset - Lower Triangle of Correlation Matrix (Pearson)<br>\"\n    \"<span style='font-size: 75%; font-weight: bold;'>\"\n    \"Here we have several strongly correlated features. \"\n    \"Are they really correlated or it's impact of outliers?</span>\",\n    yaxis_autorange=\"reversed\",\n    width=840,\n    height=840,\n)\nsave_and_show_fig(fig, \"pearson_corr_matrix\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abs_corr = (\n    lower_triu_corr.abs()\n    .unstack()\n    .sort_values(ascending=False)  # type: ignore\n    .rename(\"Absolute Pearson Correlation\")\n    .to_frame()\n    .reset_index(names=[\"Feature 1\", \"Feature 2\"])\n    .dropna()\n    .round(5)\n)\n\nwith pd.option_context(\"display.max_rows\", 10):\n    print(abs_corr)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dissimilarity = 1 - np.abs(pearson_corr)\n\nfig = ff.create_dendrogram(\n    dissimilarity,\n    labels=pearson_corr.columns,\n    orientation=\"left\",\n    colorscale=px.colors.sequential.Greys[3:],\n    # squareform() returns lower triangular in compressed form - as 1D array.\n    linkagefun=lambda x: linkage(squareform(dissimilarity), method=\"complete\"),\n)\nfig.update_xaxes(showline=False, title=\"Distance\", ticks=\"\", range=[-0.03, 1.05])\nfig.update_yaxes(showline=False, ticks=\"\")\nfig.update_layout(\n    title=\"Training Dataset - Hierarchical Clustering using Correlation Matrix (Pearson)<br>\"\n    \"<span style='font-size: 75%; font-weight: bold;'>\"\n    \"Small distance for some features. PCA maybe?</span>\",\n    height=460,\n    width=840,\n)\nfig.update_traces(line_width=1.5, opacity=1)\nsave_and_show_fig(fig, \"hierarchical_clustering\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Correlation Matrix &amp; Hierarchical Clustering</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Pearson matrix correlation shows several highly correlated pairs (five to be exact when considering threshold > $0.70$). I also checked Spearman correlations (which are based on ranks), and there are ten correlated pairs with a threshold > $0.70$. So, this dataset is more often characterized by non-linear dependencies rather than linear. Target variable correlates the most with <code>allelectrons_Average</code> and <code>atomicweight_Average</code>.<br><br>\n    In hierarchical clustering, we had to provide dissimilarity (distance) between features. Basically, we can treat dissimilarity as $\\textrm{dissimilarity} = 1 - |\\textrm{correlation}|$, and that's all. This type of visualization may come in handy to assess whether a specific dimensionality reduction algorithm like PCA may be helpful. In this case, indeed, there is an obvious pair to be reduced: <code>allelectrons_Average</code> and <code>atomicweight_Average</code>. Perhaps features at the top of the plot also can be reduced. Let's have a look at scatter plots.\n</p>","metadata":{}},{"cell_type":"code","source":"n_cols, n_features = 3, 6\nn_rows, axes = get_n_rows_and_axes(n_features, n_cols)\n\nfig = make_subplots(\n    rows=n_rows,\n    cols=n_cols,\n    horizontal_spacing=0.1,\n    vertical_spacing=0.15,\n)\n\nfor (row, col), (feature1, feature2, corr) in zip(axes, abs_corr[:n_features].to_numpy()):\n    fig.add_scatter(\n        x=train[feature1],\n        y=train[feature2],\n        mode=\"markers\",\n        name=\"\",\n        row=row,\n        col=col,\n    )\n    fig.update_xaxes(title_text=feature1, row=row, col=col)\n    fig.update_yaxes(title_text=feature2, row=row, col=col)\n\nfig.update_layout(\n    title=\"Training Dataset - Highly Linear Correlated Pairs<br>\"\n    \"<span style='font-size: 75%; font-weight: bold;'>\"\n    \"Actual high correlation corresponds to the first pair only (perhaps the second too)</span>\",\n    width=840,\n    height=540,\n    showlegend=False,\n)\nfig.update_traces(\n    marker=dict(size=1, symbol=\"x-thin\", line=dict(width=1.5, color=COLOR_SCHEME[0])),\n)\nsave_and_show_fig(fig, \"highly_correlated_scatter_plots\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Highly Linear Correlated Pairs</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    So, as we can see, there is a clearly visible high correlation between <code>allelectrons_Average</code> and <code>atomicweight_Average</code>. On the other hand, it's hard to say something about <code>ionenergy_Average</code> and <code>el_neg_chi_Average</code> because zero-points warp the correlation.\n</p>","metadata":{}},{"cell_type":"code","source":"n_cols = 3\nn_rows, axes = get_n_rows_and_axes(len(features), n_cols)\n\nfig = make_subplots(\n    rows=n_rows,\n    cols=n_cols,\n    y_title=\"Hardness - Target Variable\",\n    horizontal_spacing=0.07,\n    vertical_spacing=0.1,\n)\nfig.update_annotations(font_size=14, yshift=-45)\n\nfor (row, col), feature in zip(axes, features):\n    fig.add_scatter(\n        x=train[feature],\n        y=train.Hardness,\n        mode=\"markers\",\n        name=feature,\n        row=row,\n        col=col,\n    )\n    fig.update_xaxes(\n        title_text=f\"<b>{feature}</b>\",\n        row=row,\n        col=col,\n    )\n    if not col == 1:\n        fig.update_yaxes(showticklabels=False, row=row, col=col)\n\nfig.update_layout(\n    title=\"Training Dataset - Hardness vs Remaining Features<br>\"\n    \"<span style='font-size: 75%; font-weight: bold;'>\"\n    \"Lack of clear dependencies at first sight</span>\",\n    width=840,\n    height=840,\n    showlegend=False,\n)\nfig.update_traces(\n    marker=dict(size=1, symbol=\"x-thin\", line=dict(width=1.5, color=COLOR_SCHEME[0])),\n)\nsave_and_show_fig(fig, \"scatter_plots\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Scatter Plots</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    So, at first sight, there are no clear dependencies between target variables and the other features. We can say it only for <code>allelectrons_Average</code>, <code>atomicweight_Average</code> and <code>density_Average</code>. In these cases, actually, the negative correlation is visible, but it's not so strong. As for the whole, we can see structurally deployed samples and straight patterns. That's the result of the low diversity of continuous variables and many repetitive values. Probably we can clip outliers in <code>allelectrons_Total</code> and <code>density_Total</code>.\n</p>","metadata":{}},{"cell_type":"markdown","source":"## <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">1.3</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Probability Plots &amp; Example Transformations</span></b><a class=\"anchor\" id=\"probability_plots_and_example_transformations\"></a> [↑](#top)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    This subsection focuses on the exploration of probability plots, which are a graphical technique used to determine if a variable adheres to a particular distribution, specifically the normal distribution in this case. <b>Probability plots display samples that follow a normal distribution along a straight diagonal line.</b> Some machine learning models make the assumption that variables follow a normal distribution. Consequently, the mentioned technique assists in determining the necessary transformations to improve the variable's alignment with that distribution. We will begin with examining the original values and observing the outcomes.\n</p>","metadata":{}},{"cell_type":"code","source":"n_cols = 3\nn_rows, axes = get_n_rows_and_axes(len(features), n_cols)\n\nfig = make_subplots(\n    rows=n_rows,\n    cols=n_cols,\n    y_title=\"Observed Values\",\n    x_title=\"Theoretical Quantiles\",\n    horizontal_spacing=0.1,\n    vertical_spacing=0.1,\n)\nfig.update_annotations(font_size=14, yshift=-45)\n\nfor (row, col), feature in zip(axes, features):\n    (osm, osr), (slope, intercept, R) = stats.probplot(train[feature].dropna(), rvalue=True)\n    x_theory = np.array([osm[0], osm[-1]])\n    y_theory = intercept + slope * x_theory\n    R2 = f\"R\\u00b2 = {R * R:.2f}\"\n    fig.add_scatter(x=osm, y=osr, mode=\"markers\", row=row, col=col, name=feature)\n    fig.add_scatter(x=x_theory, y=y_theory, mode=\"lines\", row=row, col=col)\n    fig.add_annotation(\n        x=-1.25,\n        y=osr[-1] * 0.95,\n        text=R2,\n        showarrow=False,\n        row=row,\n        col=col,\n        font_size=11,\n    )\n    fig.update_xaxes(\n        title_text=f\"<b>{feature}</b>\",\n        row=row,\n        col=col,\n    )\n\nfig.update_layout(\n    title=\"Training Dataset - Probability Plots against Normal Distribution<br>\"\n    \"<span style='font-size: 75%; font-weight: bold;'>\"\n    \"Results in 'allelectrons_Total' and 'density_Total' are perturbed by some outliers</span>\",\n    width=840,\n    height=840,\n    showlegend=False,\n)\nfig.update_traces(\n    marker=dict(size=1, symbol=\"x-thin\", line=dict(width=2, color=COLOR_SCHEME[2])),\n    line_color=COLOR_SCHEME[0],\n)\nsave_and_show_fig(fig, \"probability_plots\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Probability Plots against Normal Distribution</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    As you can see, some variables fit a normal distribution well, which manifests by a high coefficient of determination (R-squared) and evenly deployed samples around the straight line. However, there are same of features which have a poor fit. We can improve that through specific transformations. Mostly used transformations are log-level and square-root ones. These work fine with right-skewed data and help to reduce the impact of outliers. Another transformation is, for example, a reciprocal one, which is sometimes used when data is skewed, or there are obvious outliers. More sophisticated methods are Box-Cox transformation (requires strictly positive numbers) and Yeo-Johnson (variation of Box-Cox), which has no restrictions concerning numbers. We will check three of mentioned: log-level, square-root and Yeo-Johnson. For this case, we will utilise the <code>probplot</code> function from the <code>scipy</code> module to get R-squared coefficients as earlier.\n</p>","metadata":{}},{"cell_type":"code","source":"r2_scores = pd.DataFrame(index=(\"Original\", \"YeoJohnson\", \"Log\", \"Sqrt\"))\n\nfor feature in features:\n    orig = train[feature].dropna()\n    _, (*_, R_orig) = stats.probplot(orig, rvalue=True)\n    _, (*_, R_yeojohn) = stats.probplot(stats.yeojohnson(orig)[0], rvalue=True)\n    _, (*_, R_log) = stats.probplot(np.log1p(orig), rvalue=True)\n    _, (*_, R_sqrt) = stats.probplot(np.sqrt(orig), rvalue=True)\n\n    r2_scores[feature] = (\n        R_orig * R_orig,\n        R_yeojohn * R_yeojohn,\n        R_log * R_log,\n        R_sqrt * R_sqrt,\n    )\n\nr2_scores = r2_scores.transpose()\nr2_scores[\"Winner\"] = r2_scores.idxmax(axis=1)\nget_pretty_frame(r2_scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>R-squared Scores within Some Transformations</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Well, as you can see <b>Yeo-Johnson's transformation wins in all cases and improves fit to the normal distribution pretty well</b> ($R^2$ scores almost in all cases are above $0.90$). However, there is a one feature where none of the transformations helps, i.e. <code>zaratio_Average</code>. Probably it's the effect of specific shape of this feature (looks like a half-normal distribution). Let's see how Yeo-Johnson transformation helps with specific feature, for example, the <code>density_Total</code> one.\n</p>","metadata":{}},{"cell_type":"code","source":"density_Total_transformed = stats.yeojohnson(train.density_Total.dropna())[0]\n(osm, osr), (slope, intercept, R) = stats.probplot(density_Total_transformed, rvalue=True)\nx_theory = np.array([osm[0], osm[-1]])\ny_theory = intercept + slope * x_theory\n\nfig = make_subplots(\n    rows=1,\n    cols=2,\n    subplot_titles=[\"Probability Plot against Normal Distribution\", \"Distribution\"],\n    horizontal_spacing=0.15,\n)\n\nfig.add_scatter(x=osm, y=osr, mode=\"markers\", row=1, col=1, name=\"YeoJohnson(density_Total)\")\nfig.add_scatter(x=x_theory, y=y_theory, mode=\"lines\", row=1, col=1)\nfig.add_annotation(\n    x=-1.25,\n    y=osr[-1] * 0.75,\n    text=f\"R\\u00b2 = {R * R:.3f}\",\n    showarrow=False,\n    row=1,\n    col=1,\n)\nfig.update_yaxes(title_text=\"Observed Values\", row=1, col=1)\nfig.update_xaxes(title_text=\"Theoretical Quantiles\", row=1, col=1)\nfig.update_traces(\n    marker=dict(size=1, symbol=\"x-thin\", line=dict(width=2, color=COLOR_SCHEME[2])),\n    line_color=COLOR_SCHEME[0],\n)\n\nfig.add_histogram(\n    x=density_Total_transformed,\n    xbins=go.histogram.XBins(size=0.1),\n    marker_color=COLOR_SCHEME[0],\n    name=\"YeoJohnson(density_Total)\",\n    histnorm=\"probability density\",\n    row=1,\n    col=2,\n)\nfig.update_yaxes(title_text=\"Probability Density\", row=1, col=2)\nfig.update_xaxes(title_text=\"YeoJohnson(density_Total)\", row=1, col=2)\n\nfig.update_layout(\n    title=\"Yeo-Johnson Transformation for 'density_Total' Feature\",\n    showlegend=False,\n    width=840,\n    height=460,\n    bargap=0.2,\n)\nfig.update_annotations(font_size=14)\nsave_and_show_fig(fig, \"density_Total_after_transform\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Yeo-Johnson Transformation</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    It looks pretty well. We can include this type of transform in the preprocessing pipeline. Models like SVM and Linear Regression should be grateful.\n</p>","metadata":{}},{"cell_type":"markdown","source":"# <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">2</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Features Importance</span></b><a class=\"anchor\" id=\"features_importance\"></a> [↑](#top)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>About Section</b> 💡\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    In this section, we will tackle the general problem of features' importance. Generally, sometimes not all variables are crucial during the training process, and only some are relevant for specific models. There are many methods, from selecting some top features based on the ANOVA test, mutual information, up to recursive feature selection with cross-validation. We can also select features from given models like random forest using the importance ratio. Generally, different methods may give different results. Moreover, it's good to include random variables in training data and measure their importance. <b>If some random numbers are more important than given features, it means that those variables are useless (may introduce a noise) from the problem perspective (but still can be useful in other tasks).</b> In this section, we will investigate decision process in a simple decision tree, and the we will focus on gradient boosting trees. We will see permutation tests, random variables, mutual information, one-way partial dependence plots (PDPs) and two-way PDPs.<br><br>\n    <b>This section has a showcase character and all what can you see here may be slightly different depending on used machine learning algorithm or random seeds. Nevertheless, it will show us, in general, which features are more important and which are not.</b>\n</p>","metadata":{}},{"cell_type":"markdown","source":"## <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">2.1</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Decision Process in Simple Decision Tree</span></b><a class=\"anchor\" id=\"simple_decision_tree_and_its_decision_process\"></a> [↑](#top)","metadata":{}},{"cell_type":"code","source":"X = train.drop(\"Hardness\", axis=1)\ny = train.Hardness\n\nDefaultDecisionTreeRegressor = partial(\n    DecisionTreeRegressor,\n    criterion=\"absolute_error\",  # Watch out on learning time complexity.\n    random_state=42,\n    max_depth=3,\n)\n\ntree = DefaultDecisionTreeRegressor().fit(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(11.5, 5.5), tight_layout=True)\nplot_tree(\n    decision_tree=tree,\n    feature_names=tree.feature_names_in_.tolist(),\n    filled=False,\n    rounded=True,\n    impurity=False,\n    proportion=True,\n    node_ids=True,\n    ax=plt.gca(),\n    fontsize=11,\n)\nplt.title(\"Decision Process in Decision Tree (depth = 3)\")\nplt.savefig(\"images/decision_process_in_tree\")\nplt.show()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for depth in range(2, 7):\n    tree.set_params(max_depth=depth).fit(X, y)\n    considered_features = tree.tree_.feature[tree.tree_.feature != -2]  # type: ignore # -2 means a leaf\n    used_features = np.unique(considered_features)\n    used_features = X.columns[used_features].to_list()\n    print(CLR + f\"Features at depth {depth}: {RED}{len(used_features):<5}\", end=\"\")\n    tree_cv_results = -cross_val_score(\n        estimator=tree,\n        X=X,\n        y=y,\n        cv=KFold(n_splits=5, shuffle=True, random_state=42),\n        scoring=\"neg_median_absolute_error\",\n        n_jobs=2,\n    )\n    mean, std = tree_cv_results.mean(), tree_cv_results.std()\n    print(CLR + \"MedAE:\", RED + f\"{mean:.2f} \\u00b1 {std:.2f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Decision Process in Tree</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    So, we've got here a simple, default decision tree within a depth of $3$. I used such a depth to easily show you nodes. Subsequently, I performed cross-validation to assess how depth influences the outcome, and it turns out that we get the same output as the depth is equal to $3$ or more. So, three features are enough to get MedAE $= 0.5$. Is that decent? Probably not. Moreover, note that number of considered features at depth $4$ increases from $3$ to $8$, compared to depth $3$. \n</p>","metadata":{}},{"cell_type":"markdown","source":"## <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">2.2</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Random Variables &amp; Permutation Test</span></b><a class=\"anchor\" id=\"random_variables_permutation_test\"></a> [↑](#top)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Here, we're going to start by introducing random variables to the dataset. Why? Well, when you introduce random variables to the dataset and train the model, you may check feature importances, for example, based on reduction in MAE criterion. <b>When a random variable contributes to the reduction of MAE more than the specific feature available in the dataset, it means that this feature is a noise indeed from the task perspective.</b>\n</p>","metadata":{}},{"cell_type":"code","source":"DefaultLGBMRegressor = partial(\n    LGBMRegressor,\n    objective=\"regression_l1\",\n    random_state=42,\n    verbose=-1,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)\nseeds = np.random.randint(0, 19937, size=5)\n\nX = train.drop(\"Hardness\", axis=1)\ny = train.Hardness\n\nlgbm = DefaultLGBMRegressor()\nimportances = []\n\nfor seed in seeds:\n    np.random.seed(seed)\n    X[\"RANDOM_1\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_2\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_3\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_4\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_5\"] = np.random.normal(size=len(X))\n\n    lgbm.set_params(random_state=seed).fit(X, y)\n    importances.append(unit_norm(lgbm.feature_importances_))\n\nimportances = (\n    pd.DataFrame({\"Feature\": X.columns, \"Importance\": np.array(importances).mean(axis=0)})\n    .sort_values(by=\"Importance\", ascending=False)\n    .reset_index(drop=True)\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(\n    importances,\n    x=\"Importance\",\n    y=\"Feature\",\n    height=460,\n    width=840,\n    title=\"Feature Importances in LGBM Regressor - Under Reduction in MAE Criterion<br>\"\n    \"<span style='font-size: 75%; font-weight: bold;'>\"\n    \"Used a default version of LGBM with L1 objective. \"\n    \"Most of features are more important than random ones</span>\",\n)\nfig.update_yaxes(categoryorder=\"total ascending\", title=\"\")\nfig.update_xaxes(range=(-0.002, 0.11))\nfig.update_traces(width=0.7)\nsave_and_show_fig(fig, \"importance_with_mae_reduction\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Feature Importances via Reduction in MAE</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Here I used $5$ different seeds to gain more reliable results. <b>As we can see, in this specific situation, most of variables are more important than random ones.</b> However, some of them are at the randomness level. What is more, if we had defined only one random feature, it could turn out that we've been just lucky that it's essential or not. We have a more general recognition when we define several of them.\n</p>","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\nseeds = np.random.randint(0, 19937, size=5)\n\nlgbm = DefaultLGBMRegressor()\npermutation_medae = defaultdict(list)\n\nfor seed in seeds:\n    np.random.seed(seed)\n    X[\"RANDOM_1\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_2\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_3\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_4\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_5\"] = np.random.normal(size=len(X))\n\n    kfold = KFold(n_splits=5, shuffle=True, random_state=seed)\n    lgbm.set_params(random_state=seed)\n\n    for k, (train_ids, valid_ids) in enumerate(kfold.split(X, y)):\n        X_train, y_train = X.iloc[train_ids], y[train_ids]  # type: ignore\n        X_valid, y_valid = X.iloc[valid_ids], y[valid_ids]  # type: ignore\n\n        lgbm.fit(X_train, y_train)\n        medae = median_absolute_error(y_valid, lgbm.predict(X_valid))  # type: ignore\n\n        for i, feature in enumerate(X_train.columns):\n            X_shuffled = X_valid.copy()\n            X_shuffled.iloc[:, i] = np.random.permutation(X_shuffled.iloc[:, i])\n            medae_shuffled = median_absolute_error(y_valid, lgbm.predict(X_shuffled))  # type: ignore\n            # I assume an increase in MedAE if the attribute is essential.\n            permutation_medae[feature].append(((medae_shuffled - medae) / medae) * 100.0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"medae_increase = (\n    pd.DataFrame(permutation_medae)\n    .mean()\n    .sort_values(ascending=False)\n    .to_frame(name=\"Mean MedAE Increase (%)\")\n    .reset_index(names=\"Feature\")\n)\n\nfig = px.bar(\n    medae_increase,\n    x=\"Mean MedAE Increase (%)\",\n    y=\"Feature\",\n    height=460,\n    width=840,\n    title=\"Mean MedAE Increase in LGBM Regressor within Samples Permutation<br>\"\n    \"<span style='font-size: 75%; font-weight: bold;'>\"\n    \"Used a default version of LGBM with L1 objective. \"\n    \"Permutation of the 'allelectrons_Average' severly punishes MedAE</span>\",\n)\nfig.update_yaxes(categoryorder=\"total ascending\", title=\"\")\nfig.update_xaxes(range=(-1, 55))\nfig.update_traces(width=0.7)\nsave_and_show_fig(fig, \"importance_with_feature_permutation\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Samples Permutation Test</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    In the code above, we explore how rearranging samples within a specific feature affects MedAE when evaluating the validation dataset. To ensure more reliable outcomes, this entire process is repeated with different random seeds. Importantly, throughout this entire process, we shuffle samples in the chosen feature of the validation subset and record results obtained from evaluating this modified dataset in a separate dictionary. If the variable is significant, we should observe worsened results in terms of MedAE. <b>As we can see, some features punish the model mostly. But it doesn't concern random variables.</b> For <code>density_Total</code>, <code>density_Average</code>, and <code>allelectrons_Total</code> variables the situation is similar to random features.<br><br>\n    Everything I show here is only illustrative, and the aim is to gain a little intuition about available features. Different models can recognise various features as relevant ones. Moreover, when you set a certain depth or minimum number of samples in a leaf in the tested gradient boosting regressor, you probably get a slightly different outcome. \n</p>","metadata":{}},{"cell_type":"markdown","source":"## <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">2.3</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Mutual Information</span></b><a class=\"anchor\" id=\"mutual_information\"></a> [↑](#top)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Let's check feature importance via the mutual information method. Generally, mutual information is a quantity that measures the relation between simultaneously sampled variables. In other words, it measures how much information about one variable is, on average, enclosed in the second variable. Intuitively, we can ask how much one variable tells us about the second one. <b>The theorem says that the mutual information between two variables is zero if and only if these are statistically independent.</b>\n</p>","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\nseeds = np.random.randint(0, 19937, size=5)\n\nscaler = StandardScaler()\nmutual_info = []\n\nfor seed in seeds:\n    np.random.seed(seed)\n    X[\"RANDOM_1\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_2\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_3\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_4\"] = np.random.normal(size=len(X))\n    X[\"RANDOM_5\"] = np.random.normal(size=len(X))\n\n    # Choose of neighbors is subjective.\n    mi = mutual_info_regression(X=scaler.fit_transform(X), y=y, n_neighbors=50, random_state=seed)\n    mutual_info.append(mi)\n\nmi_importances = (\n    pd.DataFrame({\"Feature\": X.columns, \"Mutual Information\": np.array(mutual_info).mean(axis=0)})\n    .sort_values(by=\"Mutual Information\", ascending=False)\n    .reset_index(drop=True)\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(\n    mi_importances,\n    x=\"Mutual Information\",\n    y=\"Feature\",\n    height=460,\n    width=840,\n    title=\"Feature Importances via Mutual Information<br>\"\n    \"<span style='font-size: 75%; font-weight: bold;'>\"\n    \"Used 5 different seeds. In this test all available features \"\n    \"are significantly more important than random ones</span>\",\n)\nfig.update_yaxes(categoryorder=\"total ascending\", title=\"\")\nfig.update_xaxes(range=(-0.005, 0.3))\nfig.update_traces(width=0.7)\nsave_and_show_fig(fig, \"mutual_information\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Mutual Information Results</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    As we can see, mutual information says that all features are more important than randome variables. Therefore, this is a slightly different outcome than in tests with random forest. Let's get to something more interesting, i.e. partial dependence plots.\n</p>","metadata":{}},{"cell_type":"markdown","source":"## <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">2.4</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Partial Dependence for Features of Interest</span></b><a class=\"anchor\" id=\"partial_dependence_for_features_of_interest\"></a> [↑](#top)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Generally, a partial dependence plot (PDP) is another tool for visualizing feature importance. However, this approach differs slightly from the earlier depicted. <b>Here, the partial dependence plot shows the relationship between the model outcome and a particular feature or a set of particular features.</b> In this case, the outcome (partial dependence) is an output of the <code>predict()</code> method. So, it's just a prediction for the <code>Hardness</code> variable. In other words, according to <code>scikit-learn</code> docs: <i>Intuitively, we can interpret the partial dependence as the expected target response as a function of the input features of interest.</i> Let's have a look at how it works and how it looks.\n</p>","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\n\nX = train.drop(\"Hardness\", axis=1).assign(RANDOM_1=np.random.normal(size=len(train)))\ny = train.Hardness\n\nlgbm = DefaultLGBMRegressor().fit(X, y)\n\nfig, axes = plt.subplots(4, 3, figsize=(11.5, 10), tight_layout=True, sharey=True)\nplt.suptitle(\"One-Variable Partial Dependence in LGBM Regressor\")\nPartialDependenceDisplay.from_estimator(\n    estimator=lgbm,  # type: ignore\n    X=X,\n    features=X.columns.tolist(),\n    feature_names=X.columns.tolist(),\n    response_method=\"auto\",  # In regression, the response is `predict()` output.\n    kind=\"both\",  # PDP and ICE.\n    percentiles=(0.01, 0.99),\n    subsample=0.5,\n    random_state=42,\n    n_jobs=-1,\n    ice_lines_kw={\"color\": COLOR_SCHEME[0], \"linewidth\": 0.2, \"alpha\": 0.1, \"linestyle\": \"--\"},\n    pd_line_kw={\"color\": COLOR_SCHEME[2], \"linewidth\": 2.0},\n    ax=axes.ravel(),  # type: ignore\n)\n\nfor ax in axes.ravel():\n    ax.get_legend().remove()\n    if ax not in (axes[0, 0], axes[1, 0], axes[2, 0], axes[3, 0]):\n        ax.set_ylabel(\"\")\n\nplt.savefig(\"images/one_way_partial_dependence\")\nplt.show()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Partial Dependence</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Here we have PDP visualizations all features of interest. I included one random variable to show you how looks a feature that does not drive any influence.This way, we can see how different features impact the model, whether it's a linear dependence or not. What is more, one needs to add something. <b>Actually, we've created PDP and individual conditional expectation (ICE) plots. ICE is similar to PDP, but here, the ICE plot shows the dependency of the prediction within a given feature for each sample (it means each black line corresponds to a specific sample).</b><br><br>\n    Well, so what do we see here? I describe it using the <code>ionenergy_Average</code> feature of interest. Firstly, we need to remember that interactions between features are not included here. It means that we see predictions of the model (partial dependence) depending on only one feature. Concerning the <code>ionenergy_Average</code> variable, we can see that initially the output is constant as the feature grows, i.e. the dependence is constant in the range $(8, 9)$ of <code>ionenergy_Average</code>. On a higher cut-off, the <code>ionenergy_Average</code> starts to drive an influence on the model, i.e., in the range $~9.0$ the dependence is positive and (probably?) non-linear. Next, in the range $(9, 12.0)$ it's a constant again, and subsequently has a negative impact. <b>So there are narrow ranges of this feature where the outcome rapidly grows or drops.</b><br><br>\n    Now let's have a look at random variable. The dependence is always constant here, which means that this feature doesn't introduce any information. Now let's have a look at PDP with two input features of interest. That will show us interactions.\n</p>","metadata":{}},{"cell_type":"code","source":"interaction_pair1 = [\"allelectrons_Average\", \"atomicweight_Average\"]\ninteraction_pair2 = [\"zaratio_Average\", \"ionenergy_Average\"]\n\nX = train[np.union1d(interaction_pair1, interaction_pair2)]\ny = train.Hardness\nlgbm = DefaultLGBMRegressor().fit(X, y)  # type: ignore\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(11.5, 5.5), tight_layout=True)\nplt.suptitle(\"Two-Variable PDP in LGBM Regressor\")\nPartialDependenceDisplay.from_estimator(\n    estimator=lgbm,  # type: ignore\n    X=X,\n    features=[interaction_pair1, interaction_pair2],\n    feature_names=X.columns.to_list(),\n    response_method=\"auto\",  # In regression, the response is `predict()` output.\n    percentiles=(0.01, 0.99),\n    random_state=42,\n    n_jobs=-1,\n    contour_kw={\"cmap\": \"pink\"},\n    ax=axes,  # type: ignore\n)\n\nplt.savefig(\"images/two_way_partial_dependence\")\nplt.show()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Interactions</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    The two-variable PDP above shows the dependence of the target variable on joint values of two pairs, i.e. (<code>allelectrons_Average</code>, <code>atomicweight_Average</code>) and (<code>zaratio_Average</code>, <code>ionenergy_Average</code>). Here, for example, we can see that if the <code>zaratio_Average</code> is greater than $0.5$, the main impact on prediction has <code>ionenergy_Average</code>, but when <code>zaratio_Average</code> is in the range $(0.45, 0.50)$, the situation is diversed.\n</p>","metadata":{}},{"cell_type":"markdown","source":"# <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">3</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Dimensionality Reduction</span></b><a class=\"anchor\" id=\"dimensionality_reduction\"></a> [↑](#top)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    There we have several interesting dimensionality reduction algorithms, like t-SNE, PCA or LLE. For example the t-SNE method is a great reduction technique commonly used for visualizing complex and high-dimensional data in a lower-dimensional space. Moreover, it tries to preserve the original structure of data, so similar samples should be deployed close to each other in reduced dimensions. Using t-SNE we should provide scaled data, otherwise, certain dimensions can be dominated by features with larger scales or units. For the purpose of this notebook, we can test two techniques, i.e. PCA, Isomap and t-SNE.\n</p>","metadata":{}},{"cell_type":"code","source":"X = train.drop(\"Hardness\", axis=1)\ny = train.Hardness\n\ntransformer = PowerTransformer(method=\"yeo-johnson\", standardize=True)\nX_rescaled = transformer.fit_transform(X)\n\npca_2d = PCA(n_components=2, random_state=42)\niso_2d = Isomap(n_components=2, n_neighbors=20, n_jobs=-1)\ntsne_2d = TSNE(n_components=2, random_state=42, n_jobs=-1)\n\npca_2d_results = pd.DataFrame(pca_2d.fit_transform(X_rescaled), columns=(\"x1\", \"x2\")).join(y)\niso_2d_results = pd.DataFrame(iso_2d.fit_transform(X_rescaled), columns=(\"x1\", \"x2\")).join(y)\ntsne_2d_results = pd.DataFrame(tsne_2d.fit_transform(X_rescaled), columns=(\"x1\", \"x2\")).join(y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_cols, n_projections = 3, 3\nn_rows, axes = get_n_rows_and_axes(n_projections, n_cols)\nfig = make_subplots(\n    rows=n_rows,\n    cols=n_cols,\n    subplot_titles=(\"PCA\", \"Isomap\", \"TSNE\"),\n    x_title=\"x1\",\n    y_title=\"x2\",\n    # horizontal_spacing=0.1,\n    vertical_spacing=0.1,\n)\n\nfor (row, col), projection in zip(axes, (pca_2d_results, iso_2d_results, tsne_2d_results)):\n    fig.add_scatter(\n        x=projection.x1,\n        y=projection.x2,\n        mode=\"markers\",\n        marker=dict(size=1, color=projection.Hardness, coloraxis=\"coloraxis\"),\n        row=row,\n        col=col,\n        showlegend=False,\n    )\n\nfig.update_annotations(font_size=14, yshift=-15)\nfig.update_coloraxes(\n    colorbar=dict(\n        title_text=\"Hardness\",\n        ticklabelposition=\"outside bottom\",\n        orientation=\"h\",\n        title_side=\"bottom\",\n        yanchor=\"bottom\",\n        xanchor=\"center\",\n        len=1.02,\n        y=-0.5,\n        x=0.5,\n    ),\n    colorscale=colormap,\n)\nfig.update_layout(\n    title=\"Training Dataset - Dimensionality Reduction with Different Algorithms<br>\"\n    \"<span style='font-size: 75%; font-weight: bold;'>\"\n    \"TSNE (as usual) seems to create the best visuals</span>\",\n    width=840,\n    height=440,\n)\nsave_and_show_fig(fig, \"projections_2d\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Dimensionality Reduction Results</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    As we can see, PCA and Isomap produce kind similar outcomes. On the other hand, t-SNE creates a pretty cool low-dimensional visual. Let's get one step further and use t-SNE to create a 3D projection.\n</p>","metadata":{}},{"cell_type":"code","source":"tsne_3d = TSNE(n_components=3, random_state=42, n_jobs=-1)\ntsne_3d_results = pd.DataFrame(tsne_3d.fit_transform(X_rescaled), columns=(\"x1\", \"x2\", \"x3\")).join(y)\n\nfig = px.scatter_3d(\n    tsne_3d_results,\n    x=\"x1\",\n    y=\"x2\",\n    z=\"x3\",\n    color=\"Hardness\",\n    color_continuous_scale=colormap,\n    opacity=0.5,\n    height=840,\n    width=840,\n    title=\"Training Dataset - 3D Projection with t-SNE<br>\"\n    \"<span style='font-size: 75%; font-weight: bold;'>\"\n    \"Many samples overlap but these with low 'Hardness' seems to \"\n    \"be separated pretty well</span>\",\n)\nfig.update_traces(marker_size=2)\nfig.update_coloraxes(colorbar=dict(ticklabelposition=\"outside bottom\"))\nfig.show()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Dimensionality Reduction with t-SNE</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    So, here we see that t-SNE tried to separate samples with different <code>Hardness</code> value levels. For example, there is an area that is dominated by samples with low <code>Hardness</code>. Well, dimensionality reduction is a nice tool but in this specific problem samples seems to be difficult to separate them clearly.\n</p>","metadata":{}},{"cell_type":"markdown","source":"# <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">4</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Outliers Detection</span></b><a class=\"anchor\" id=\"outliers_detection\"></a> [↑](#top)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Generally, there are several methods available in the <code>scikit-learn</code> library if you ask for the outlier detector, such as <code>SGDOneClassSVM</code>, which is a linear model, <code>LocalOutlierFactor</code>, which is based on nearest neighbours, and <code>IsolationForest</code> based on bagging trees. We will focus on the <code>LocalOutlierFactor</code>, but I provide a code that you can easily modify to change a detector.<br><br>\n    Okay, so how do we assess whether removing outliers will probably work for the test dataset? Well, let's cross-validate it. The point is that we carry on typical cross-validation, but in addition to train the model on the training subset, we train it again on the \"clean\" training subset, where \"clean\" means a subset without outliers. Subsequently, we collect results obtained for validation subsets for two types of models. This trained on full training subset and that trained on clean subset. Suppose we observe a reduction in terms of a given metric for the validation subset in all folds. In that case, that's probably a good method of removing outliers. And here is a crucial point. We should observe a reduction in all folds. <b>When we observe a reduction in some folds but not in others, we rather should not remove outliers because the test dataset can resemble that one fold when removing outliers came with lower model performance.</b>\n</p>","metadata":{}},{"cell_type":"code","source":"def remove_outliers(data, detector):\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(f\"'data' must be {pd.DataFrame!r} instance\")\n\n    result = detector.fit_predict(data)\n    outlier_ids = pd.Series(result == -1, index=data.index, dtype=bool)\n    data_ids = pd.Series(np.ones_like(data.index), index=data.index, dtype=bool)\n    \n    return data[~(outlier_ids & data_ids)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Removing Outliers</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Since outlier detectors in <code>scikit-learn</code> predict $-1$ for the outlier sample, we create a mask and return the dataset without associated observations in the <code>remove_outliers()</code> function. Subsequently, below I create simple LGBM model which implements MAE loss pretty well compared to previously used random forests, and perform CV scheme I described earlier.\n</p>","metadata":{}},{"cell_type":"code","source":"lgbm = DefaultLGBMRegressor()\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\ndetector = make_pipeline(\n    PowerTransformer(method=\"yeo-johnson\", standardize=True),\n    LocalOutlierFactor(),\n)\n\nhyperparameter = \"localoutlierfactor__contamination\"\nhyperparameter_values = [None] + np.arange(0.01, 0.15, 0.01).tolist()\nno_outliers_medae = {}\n\nfor k, (train_ids, valid_ids) in enumerate(kfold.split(X, y), start=1):\n    X_train, y_train = X.iloc[train_ids], y.iloc[train_ids]\n    X_valid, y_valid = X.iloc[valid_ids], y.iloc[valid_ids]\n\n    lgbm.fit(X_train, y_train)\n    default_medae = median_absolute_error(y_valid, lgbm.predict(X_valid))  # type:ignore\n\n    for hp_value in hyperparameter_values:\n        if hp_value is None:\n            no_outliers_medae[f\"0 - {k}\"] = default_medae\n            continue\n\n        detector.set_params(**{hyperparameter: hp_value})\n        X_no_outliers = remove_outliers(X_train, detector)\n        y_no_outliers = y_train[X_no_outliers.index]\n\n        lgbm.fit(X_no_outliers, y_no_outliers)\n        clean_medae = median_absolute_error(y_valid, lgbm.predict(X_valid))  # type:ignore\n        no_outliers_medae[f\"{hp_value} - {k}\"] = clean_medae","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detector_medae = pd.DataFrame({\"KEY\": no_outliers_medae.keys(), \"MedAE\": no_outliers_medae.values()})\ndetector_medae[[hyperparameter, \"Fold\"]] = detector_medae.KEY.str.split(\"-\", expand=True)\ndefault_medae = detector_medae[detector_medae[hyperparameter].astype(float) == 0].MedAE\n\nfig = px.line(\n    detector_medae,\n    x=hyperparameter,\n    y=\"MedAE\",\n    facet_row=\"Fold\",\n    facet_row_spacing=0.07,\n    color_discrete_sequence=COLOR_SCHEME[2:],\n    height=640,\n    width=840,\n    title=f\"Influence of '{hyperparameter}' Hyperparameter on MedAE in LGBM<br>\"\n    \"<span style='font-size: 75%; font-weight: bold;'>\"\n    f\"None of the '{hyperparameter}' values causes \"\n    \"a reduction in MedAE in all folds</span>\",\n)\nfor fold, fold_default_medae in enumerate(default_medae):\n    fig.add_hline(\n        fold_default_medae,\n        annotation_text=f\"<b>Default MedAE: {fold_default_medae:.3f}</b>\",\n        annotation_position=\"bottom left\",\n        annotation_font_size=12,\n        line_width=1.5,\n        opacity=0.75,\n        line_dash=\"dot\",\n        line_color=COLOR_SCHEME[0],\n        row=len(default_medae) - fold,  # type:ignore\n    )\nfig.update_traces(line_width=2)\nfig.update_layout(margin_pad=10)\nfig.update_xaxes(tickformat=\".2f\", type=\"linear\")\nsave_and_show_fig(fig, \"outlier_detection\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Removing Outliers vs Performance</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Unfortunately, removing outliers didn't reduce MedAE in all folds for any of the <code>contamination</code> values in the <code>LocalOutlierFactor</code> detector. I also checked <code>IsolationForest</code> and <code>SGDOneClassSVM</code> with several hyperparameter combinations, but the result is similar. Of course, the whole process depends on model combination. Perhaps this method works for other regressor, rather than LGBM.\n</p>","metadata":{}},{"cell_type":"markdown","source":"# <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">5</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Modelling</span></b><a class=\"anchor\" id=\"modelling\"></a> [↑](#top)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    In this section, I'm going to try different approaches to this competition.\n</p>","metadata":{}},{"cell_type":"markdown","source":"## <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">5.1</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Regression Approach</span></b><a class=\"anchor\" id=\"regression_approach\"></a> [↑](#top)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Let's get started with a typical approach, i.e. a regression problem. Below, you can see several preprocessing steps.\n</p>","metadata":{}},{"cell_type":"code","source":"def clip_numeric_feature(X, lower_bound=None, upper_bound=None):\n    X = np.array(X, copy=False)\n    return np.clip(X[:, [0]], lower_bound, upper_bound)\n\n\ndef features_interaction(X, op=\"mul\", eps=1e-9):\n    ops = {\n        \"mul\": operator.mul,\n        \"truediv\": operator.truediv,\n        \"add\": operator.add,\n        \"sub\": operator.sub,\n    }\n    X = np.array(X, copy=False)\n    return ops.get(op, operator.mul)(X[:, [0]], X[:, [1]] + eps)\n\n\ndef interaction_name(function_transformer, feature_names_in, name):\n    return [name]  # feature names out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Preprocessing Functions</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    There, we have some simple functions. The first one just clips a numeric feature to given bounds. The second one adds an interaction between two features.  \n</p>","metadata":{}},{"cell_type":"code","source":"preprocessing = make_pipeline(\n    make_column_transformer(\n        (\n            FunctionTransformer(\n                features_interaction,\n                feature_names_out=partial(\n                    interaction_name, name=\"allelectrons_atomicweight_avg_mul\"\n                ),\n                kw_args=dict(op=\"mul\"),\n            ),\n            [\"allelectrons_Average\", \"atomicweight_Average\"],\n        ),\n        (\n            FunctionTransformer(\n                clip_numeric_feature,\n                feature_names_out=\"one-to-one\",\n                kw_args=dict(lower_bound=0, upper_bound=1894),\n            ),\n            [\"allelectrons_Total\"],\n        ),\n        (\n            FunctionTransformer(\n                clip_numeric_feature,\n                feature_names_out=\"one-to-one\",\n                kw_args=dict(lower_bound=0, upper_bound=236),\n            ),\n            [\"density_Total\"],\n        ),\n        remainder=\"passthrough\",\n        verbose_feature_names_out=True,\n    ),\n    PowerTransformer(method=\"yeo-johnson\", standardize=True),\n    # FeatureAgglomeration(n_clusters=3, linkage=\"complete\"),\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Preprocessing Pipeline</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Here, we have a simple preprocessing pipeline that clips some features with outliers we saw earlier and adds a multiplication of the highest correlated pair.\n</p>","metadata":{}},{"cell_type":"code","source":"class FeatureFromModel(\n    MetaEstimatorMixin,  # Accepts any regressor\n    BaseEstimator,  # `set_params()` and `get_params()`\n    TransformerMixin,  # `fit_transform()`\n    ClassNamePrefixFeaturesOutMixin,  # `get_feature_names_out()`\n):\n    def __init__(self, estimator):\n        self.estimator = estimator\n\n    def fit(self, X, y):\n        check_array(X)\n        self.estimator_ = clone(self.estimator).fit(X, y)\n        self.n_features_in_ = self.estimator_.n_features_in_\n        self._n_features_out = y.ndim\n        if hasattr(self.estimator, \"feature_names_in_\"):\n            self.feature_names_in_ = self.estimator.feature_names_in_\n        return self\n\n    def transform(self, X):\n        check_is_fitted(self)\n        check_array(X)\n        y_pred = self.estimator_.predict(X)\n        if y_pred.ndim == 1:\n            return y_pred.reshape(-1, 1)\n        return y_pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Feature From Model</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    <code>FeatureFromModel</code> estimator will add a new feature from KNN, which should reflect Mohs hardness of nearest samples.\n</p>","metadata":{}},{"cell_type":"code","source":"def round_to_nearest(y, known_values=None, top_n=20):\n    if known_values is None:\n        known_values = y.value_counts().index.to_numpy()[:top_n]\n    y_repeated = np.tile(y, reps=(len(known_values), 1)).transpose()\n    lowest_diff_ids = np.abs(y_repeated - known_values).argmin(axis=1)\n    return known_values[lowest_diff_ids]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.drop(\"Hardness\", axis=1)\ny = train.Hardness\ny_oof = np.zeros_like(y)\n\nkfold = KFold(n_splits=10, shuffle=True, random_state=42)\nknn = make_pipeline(\n    FeatureFromModel(KNeighborsRegressor(n_neighbors=100)),\n    MinMaxScaler(),\n)\nlgb = LGBMRegressor(\n    objective=\"regression_l1\",\n    min_child_samples=100,\n    reg_alpha=20,\n    reg_lambda=20,\n    verbose=-1,\n)\n\nfor fold, (train_ids, valid_ids) in enumerate(kfold.split(X, y), start=1):\n    X_train, y_train = X.iloc[train_ids], y.iloc[train_ids]\n    X_valid, y_valid = X.iloc[valid_ids], y.iloc[valid_ids]\n\n    X_train = preprocessing.fit_transform(X_train)\n    X_valid = preprocessing.transform(X_valid)\n\n    X_train = np.c_[X_train, knn.fit_transform(X_train, round_to_nearest(y_train, top_n=10))]\n    X_valid = np.c_[X_valid, knn.transform(X_valid)]\n\n    lgb.fit(X_train, round_to_nearest(y_train, top_n=30))\n    y_pred = lgb.predict(X_valid).round(2)  # type: ignore\n    y_oof[valid_ids] = y_pred\n    medae = median_absolute_error(y_valid, y_pred)  # type: ignore\n\n    print(CLR + f\"Fold: {fold:2d}\", CLR + \"- MedAE:\", RED + f\"{medae:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2, horizontal_spacing=0.1)\n\nfig.add_histogram(\n    x=np.abs(y - y_oof),\n    xbins=go.histogram.XBins(size=0.2),\n    showlegend=False,\n    row=1,\n    col=1,\n)\nfig.update_xaxes(title=\"Absolute Error\", range=(-0.5, 6), row=1, col=1)\nfig.update_yaxes(title=\"Count\", row=1, col=1)\nfig.add_scatter(\n    x=y,\n    y=y_oof,\n    mode=\"markers\",\n    showlegend=True,\n    name=\"Predictions\",\n    marker=dict(symbol=\"x\", size=2, color=COLOR_SCHEME[0]),\n    row=1,\n    col=2,\n)\nfig.add_scatter(x=[1, 10], y=[1, 10], name=\"Perfectly Predicted\", mode=\"lines\", row=1, col=2)\nfig.update_yaxes(title=\"Predicted 'Hardness'\", row=1, col=2)\nfig.update_xaxes(title=\"True 'Hardness'\", row=1, col=2)\nfig.update_layout(\n    title=\"Training Dataset - Out-of-Fold Predictions via Regression Approach<br>\"\n    \"<span style='font-size: 75%; font-weight: bold;'>\"\n    \"Unfortunately nothing special</span>\",\n    height=460,\n    width=840,\n    bargap=0.2,\n    legend=dict(y=1.02, x=1),\n)\nsave_and_show_fig(fig, \"out_of_fold_preds_reg\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Simple LGBM Model CV</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    In the code above, I decided to reject some unique values of the target variable (we do this only for training subset). Subsequently, I perform a typical CV. As you can see, all of this is completely useless because we got better results with the decision tree with depth $3$. Thank you, MedAE...\n</p>","metadata":{}},{"cell_type":"code","source":"y_train = train.Hardness\nX_train = preprocessing.fit_transform(train.drop(\"Hardness\", axis=1))\nX_train = np.c_[X_train, knn.fit_transform(X_train, round_to_nearest(y_train, top_n=10))]\n\nX_test = preprocessing.transform(test)\nX_test = np.c_[X_test, knn.transform(X_test)]\n\nlgb.fit(X_train, round_to_nearest(y_train, top_n=30))\n\nsubmission = pd.DataFrame(\n    {\n        \"id\": test.index,\n        \"Hardness\": lgb.predict(X_test).round(2),  # type:ignore\n    }\n).set_index(\"id\")\n\nsubmission.to_csv(\"submission_reg.csv\")\nget_pretty_frame(submission.head(), precision=2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">5.2</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Multiclass Classification Approach</span></b><a class=\"anchor\" id=\"multiclass_classification_approach\"></a> [↑](#top)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    A couple of days ago, the user <a href=\"https://www.kaggle.com/fabienpv\">FabienPv</a> posted an interesting finding. See here: <a href=\"https://www.kaggle.com/competitions/playground-series-s3e25/discussion/457631\">Interesting fact about data categorization</a>. Namely, we will treat this competition as a multiclass classification problem with categories provided in the post above.\n</p>","metadata":{}},{"cell_type":"code","source":"X = train.drop(\"Hardness\", axis=1)\ny = train.Hardness\ny_oof = np.zeros_like(y)\n\ncats = np.array([1.75, 2.55, 3.75, 4.75, 5.75, 6.55, 7.75, 8.75, 9.75])\nencoder = LabelEncoder()\nrskf_cats = encoder.fit_transform(round_to_nearest(y, cats))\n\nlgb = LGBMClassifier(random_state=42, max_depth=3, verbose=-1)\nxgb = XGBClassifier(random_state=42, max_depth=3)\n\nrskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\nfor fold, (train_ids, valid_ids) in enumerate(rskf.split(X, rskf_cats), start=1):\n    X_train, y_train = X.iloc[train_ids], y.iloc[train_ids]\n    X_valid, y_valid = X.iloc[valid_ids], y.iloc[valid_ids]\n\n    y_train = encoder.fit_transform(round_to_nearest(y_train, cats))\n    X_train = preprocessing.fit_transform(X_train)\n    X_valid = preprocessing.transform(X_valid)\n\n    lgb.fit(X_train, y_train)  # type: ignore\n    xgb.fit(X_train, y_train)\n\n    lgb_proba = lgb.predict_proba(X_valid)\n    xgb_proba = xgb.predict_proba(X_valid)\n\n    y_cat = np.argmax(xgb_proba, axis=1)\n    y_pred = encoder.inverse_transform(y_cat)\n    y_oof[valid_ids] = y_pred\n\n    medae = median_absolute_error(y_valid, y_pred)\n    print(CLR + f\"Fold: {fold:2d}\", CLR + \"- MedAE:\", RED + f\"{medae:.3f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2, horizontal_spacing=0.1)\n\nfig.add_histogram(\n    x=np.abs(y - y_oof),\n    xbins=go.histogram.XBins(size=0.2),\n    showlegend=False,\n    row=1,\n    col=1,\n)\nfig.update_xaxes(title=\"Absolute Error\", range=(-0.5, 6), row=1, col=1)\nfig.update_yaxes(title=\"Count\", row=1, col=1)\nfig.add_scatter(\n    x=y,\n    y=y_oof,\n    mode=\"markers\",\n    showlegend=True,\n    name=\"Predictions\",\n    marker=dict(symbol=\"x\", size=3, color=COLOR_SCHEME[0]),\n    row=1,\n    col=2,\n)\nfig.add_scatter(x=[1, 10], y=[1, 10], name=\"Perfectly Predicted\", mode=\"lines\", row=1, col=2)\nfig.update_yaxes(title=\"Predicted 'Hardness'\", row=1, col=2)\nfig.update_xaxes(title=\"True 'Hardness'\", row=1, col=2)\nfig.update_layout(\n    title=\"Training Dataset - Out-of-Fold Predictions via Multiclass Classification Approach\",\n    height=460,\n    width=840,\n    bargap=0.2,\n    legend=dict(y=1.02, x=1),\n)\nsave_and_show_fig(fig, \"out_of_fold_preds_classif\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 20px;\n    font-family: 'JetBrains Mono';\n    color: #4A4B52;\n    border-bottom: 2px solid #E8BA91;\n\">\n    <b>Classification Approach</b> 📜\n</p>\n\n<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    Obviously, this approach looks much better in the CV, but what will happen for private LB?\n</p>","metadata":{}},{"cell_type":"code","source":"cats = np.array([1.75, 2.55, 3.75, 4.75, 5.75, 6.55, 7.75, 8.75, 9.75])\nencoder = LabelEncoder()\n\nX_train = preprocessing.fit_transform(train.drop(\"Hardness\", axis=1))\nX_test = preprocessing.transform(test)\ny_train = encoder.fit_transform(round_to_nearest(train.Hardness, cats))\n\nlgb = LGBMClassifier(random_state=42, max_depth=3, verbose=-1)\nxgb = XGBClassifier(random_state=42, max_depth=3)\n\nlgb.fit(X_train, y_train)  # type: ignore\nxgb.fit(X_train, y_train)\n\nlgb_proba = lgb.predict_proba(X_test)\nxgb_proba = xgb.predict_proba(X_test)\n\ny_cat = np.argmax(lgb_proba + xgb_proba, axis=1)\ny_pred = encoder.inverse_transform(y_cat)\n\nsubmission = pd.DataFrame(\n    {\n        \"id\": test.index,\n        \"Hardness\": y_pred,\n    }\n).set_index(\"id\")\n\nsubmission.to_csv(\"submission_classif.csv\")\nget_pretty_frame(submission.head(), precision=2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">6</span> <span style='color: #E8BA91'>|</span> <span style=\"font-family: 'JetBrains Mono'; color: #4A4B52\">Summary</span></b><a class=\"anchor\" id=\"summary\"></a> [↑](#top)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"\n    font-size: 16px;\n    font-family: 'JetBrains Mono';\n    text-align: justify;\n    text-justify: inter-word;\n\">\n    That's all in this notebook, and if you read this, leave an upvote if you like it. Good luck!\n</p>","metadata":{}}]}