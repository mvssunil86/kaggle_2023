{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":60892,"databundleVersionId":6989718,"sourceType":"competition"},{"sourceId":6497264,"sourceType":"datasetVersion","datasetId":3755317},{"sourceId":6612067,"sourceType":"datasetVersion","datasetId":3815527},{"sourceId":6908126,"sourceType":"datasetVersion","datasetId":3856023},{"sourceId":6973306,"sourceType":"datasetVersion","datasetId":4000165},{"sourceId":151101795,"sourceType":"kernelVersion"},{"sourceId":151065289,"sourceType":"kernelVersion"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:black; font-size:150%; text-align:left;padding:3.0px; background: #cceeff; border-bottom: 8px solid #004466\" > TABLE OF CONTENTS<br><div>  \n* [IMPORTS](#1)\n* [INTRODUCTION](#2)\n    * [CONFIGURATION](#2.1)\n    * [CONFIGURATION PARAMETERS](#2.2)    \n    * [DATASET COLUMNS](#2.3)\n* [PREPROCESSING](#3)\n* [ADVERSARIAL CV](#4)\n* [EDA AND VISUALS](#5) \n* [DATA TRANSFORMS](#6)\n* [MODEL TRAINING](#7)    \n* [ENSEMBLE AND SUBMISSION](#8)  \n* [PLANNED WAY FORWARD](#9)     ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:black; font-size:120%; text-align:left;padding:3.0px; background: #cceeff; border-bottom: 8px solid #004466\" > IMPORTS<br> <div> ","metadata":{}},{"cell_type":"code","source":"%%time \n#https://www.kaggle.com/ravi20076\n\n# Installing select libraries:-\nfrom gc import collect;\nfrom warnings import filterwarnings;\nfilterwarnings('ignore');\nfrom IPython.display import clear_output;\n\n!pip install -q category_encoders;\n!pip install -q /kaggle/input/lightgbm410/lightgbm-4.1.0-py3-none-manylinux_2_28_x86_64.whl;\n!pip install -q /kaggle/input/xgboost-2-0-0-whl/xgboost-2.0.1-py3-none-manylinux2014_x86_64.whl;\n!pip install -q /kaggle/input/sklegowheelfile/autograd-1.6.2-py3-none-any.whl;\n!pip install -q /kaggle/input/sklegowheelfile/scikit_lego-0.6.16-py2.py3-none-any.whl;\n\nclear_output();\n\nimport xgboost as xgb, lightgbm as lgb, sklego;\nprint(f\"---> Current XGBoost = {xgb.__version__} | LightGBM = {lgb.__version__} | SKLego = {sklego.__version__}\");\nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:47:09.398828Z","iopub.execute_input":"2023-11-18T11:47:09.39923Z","iopub.status.idle":"2023-11-18T11:49:53.524989Z","shell.execute_reply.started":"2023-11-18T11:47:09.399184Z","shell.execute_reply":"2023-11-18T11:49:53.523821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# General library imports:-\nfrom copy import deepcopy;\nimport pandas as pd;\nimport numpy as np;\nimport re;\nfrom scipy.stats import mode, kstest, normaltest, shapiro, anderson, jarque_bera;\nfrom collections import Counter;\nfrom itertools import product;\nfrom colorama import Fore, Style, init;\nfrom warnings import filterwarnings;\nfilterwarnings('ignore');\nimport joblib;\nimport os;\n\nfrom tqdm.notebook import tqdm;\nimport seaborn as sns;\nimport matplotlib.pyplot as plt;\nfrom matplotlib.colors import ListedColormap as LCM;\n%matplotlib inline\n\nfrom pprint import pprint;\n\nprint();\ncollect();\nclear_output();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:53.526745Z","iopub.execute_input":"2023-11-18T11:49:53.527192Z","iopub.status.idle":"2023-11-18T11:49:53.802683Z","shell.execute_reply.started":"2023-11-18T11:49:53.527157Z","shell.execute_reply":"2023-11-18T11:49:53.801682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\n# Importing model and pipeline specifics:-\nfrom category_encoders import OrdinalEncoder, OneHotEncoder;\n\n# Pipeline specifics:-\nfrom sklearn.preprocessing import (RobustScaler, \n                                   MinMaxScaler, \n                                   StandardScaler, \n                                   FunctionTransformer as FT,\n                                   PowerTransformer,\n                                  );\nfrom sklearn.impute import SimpleImputer as SI;\nfrom sklearn.model_selection import (RepeatedStratifiedKFold as RSKF, \n                                     StratifiedKFold as SKF,\n                                     KFold, \n                                     RepeatedKFold as RKF, \n                                     cross_val_score, cross_val_predict\n                                    );\nfrom sklearn.inspection import permutation_importance;\nfrom sklearn.feature_selection import mutual_info_classif, RFE;\nfrom sklearn.pipeline import Pipeline, make_pipeline;\nfrom sklearn.base import BaseEstimator, TransformerMixin;\nfrom sklearn.compose import ColumnTransformer;\n\n# ML Model training:-\nfrom sklearn.metrics import median_absolute_error as medae,  make_scorer;\nfrom xgboost import DMatrix, XGBRegressor;\nfrom lightgbm import LGBMRegressor, log_evaluation, early_stopping, LGBMClassifier;\nfrom catboost import CatBoostRegressor, Pool;\nfrom sklearn.ensemble import (RandomForestRegressor as RFR, \n                              ExtraTreesRegressor as ETR,\n                              AdaBoostRegressor as ABR,\n                              BaggingRegressor as BR,\n                              HistGradientBoostingRegressor as HGBR,\n                              GradientBoostingRegressor as GBR,\n                             );\nfrom sklearn.neighbors import KNeighborsRegressor as KNNR;\nfrom sklearn.svm import SVR;\nfrom sklearn.linear_model import Ridge as RIDGER;\n\n# Ensemble and tuning:-\nimport optuna;\nfrom optuna import Trial, trial, create_study;\nfrom optuna.samplers import TPESampler, CmaEsSampler;\noptuna.logging.set_verbosity = optuna.logging.ERROR;\n\nfrom sklego.linear_model import LADRegression as LADR;\n\nclear_output();\nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:53.8064Z","iopub.execute_input":"2023-11-18T11:49:53.806755Z","iopub.status.idle":"2023-11-18T11:49:56.098148Z","shell.execute_reply.started":"2023-11-18T11:49:53.806723Z","shell.execute_reply":"2023-11-18T11:49:56.097096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\n# Setting rc parameters in seaborn for plots and graphs- \n# Reference - https://matplotlib.org/stable/tutorials/introductory/customizing.html:-\n# To alter this, refer to matplotlib.rcParams.keys()\n\nsns.set({\"axes.facecolor\"       : \"#ffffff\",\n         \"figure.facecolor\"     : \"#ffffff\",\n         \"axes.edgecolor\"       : \"#000000\",\n         \"grid.color\"           : \"#ffffff\",\n         \"font.family\"          : ['Cambria'],\n         \"axes.labelcolor\"      : \"#000000\",\n         \"xtick.color\"          : \"#000000\",\n         \"ytick.color\"          : \"#000000\",\n         \"grid.linewidth\"       : 0.75,  \n         \"grid.linestyle\"       : \"--\",\n         \"axes.titlecolor\"      : '#0099e6',\n         'axes.titlesize'       : 8.5,\n         'axes.labelweight'     : \"bold\",\n         'legend.fontsize'      : 7.0,\n         'legend.title_fontsize': 7.0,\n         'font.size'            : 7.5,\n         'xtick.labelsize'      : 7.5,\n         'ytick.labelsize'      : 7.5,        \n        });\n\n# Color printing    \ndef PrintColor(text:str, color = Fore.BLUE, style = Style.BRIGHT):\n    \"Prints color outputs using colorama using a text F-string\";\n    print(style + color + text + Style.RESET_ALL); \n\n# Making sklearn pipeline outputs as dataframe:-\nfrom sklearn import set_config; \nset_config(transform_output = \"pandas\");\npd.set_option('display.max_columns', 50);\npd.set_option('display.max_rows', 50);\n\nprint();\ncollect();\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:56.101404Z","iopub.execute_input":"2023-11-18T11:49:56.101963Z","iopub.status.idle":"2023-11-18T11:49:56.241408Z","shell.execute_reply.started":"2023-11-18T11:49:56.10192Z","shell.execute_reply":"2023-11-18T11:49:56.23981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:black; font-size:120%; text-align:left;padding:3.0px; background: #cceeff; border-bottom: 8px solid #004466\" > INTRODUCTION<br><div> ","metadata":{}},{"cell_type":"markdown","source":"| Version<br>Number | Version Details | CV score| Single/ Ensemble|Public LB Score|\n| :-: | --- | :-: | :-: |:-:|\n| **V1** |* No scaling and null treatment <br> * No secondary features <br> * Used original data <br> * 5 x 2 RKF CV <br> * Tree based ML models and Optuna ensemble|0.543611|Optuna blend |0.51966|\n| **V2** |* No scaling and null treatment <br> * No secondary features <br> * Used original data <br> * Cleaned synthetic targets with target-scaling <br> * 5 x 2 RSKF CV with target encoding <br> * Tree based ML models and LAD ensemble|0.516996|LAD |0.49161|\n| **V3** |* No secondary features <br> * Used original data <br> * Cleaned synthetic targets and dropped duplicates in train set <br> * 5 x 2 RKF CV<br> * ML Models and LAD ensemble <br> * Post-processed with train-test duplicates|0.52585|LAD |0.49396|\n| **V4** |* Same as V3 <br> * Used target encoding and RSKF 5x2|0.52429|LAD |0.50145|\n| **V5** |* Same as V3 <br> * Used KFold 10x1 <br> * Used better public score notebooks for blend|0.519862|LAD |0.43467|\n| **V6** |* Used KFold 10x1 <br> * Removed LightGBM from ML models <br> * Used quantile and MedAE objective functions <br> * Used better public score notebooks for blend|0.52037|LAD |0.41890|\n| **V7** |* Used KFold 10x1 <br> * Used log and square based on skewness <br> * Used quantile and MedAE objective functions <br> * Used better public score notebooks for blend||LAD ||","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.1\"></a>\n## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #0059b3; border-bottom: 8px solid #e6e6e6\" > CONFIGURATION<br><div> ","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Configuration class:-\nclass CFG:\n    \"Configuration class for parameters and CV strategy for tuning and training\";\n    \n    # Data preparation:-   \n    version_nb         = 7;\n    test_req           = \"N\";\n    test_sample_frac   = 0.025;\n    gpu_switch         = \"OFF\"; \n    state              = 42;\n    target             = 'Hardness';\n    episode            = 25;\n    path               = f\"/kaggle/input/playground-series-s3e{episode}\";\n    orig_path          = f\"/kaggle/input/prediction-of-mohs-hardness-with-machine-learning/jm79zfps6b-1/Mineral_Dataset_Supplementary_Info.csv\";\n    \n    dtl_preproc_req    = \"Y\";\n    adv_cv_req         = \"N\";\n    ftre_plots_req     = 'N';\n    ftre_imp_req       = \"N\";\n    \n    # Data transforms and scaling:-    \n    conjoin_orig_data  = \"Y\";\n    drop_nulls         = \"Y\";\n    sec_ftre_req       = \"N\";\n    scale_req          = \"N\";\n    # NOTE---Keep a value here even if scale_req = N, this is used for linear models:-\n    scl_method         = \"PT\"; \n    enc_method         = 'Label';\n    OH_cols            = [];\n    tgt_scl_fct        = 100;\n    \n    # Model Training:- \n    baseline_req       = \"N\";\n    pstprcs_oof        = \"Y\";\n    pstprcs_train      = \"Y\";\n    pstprcs_test       = \"Y\";\n    ML                 = \"Y\";\n    \n    pseudo_lbl_req     = \"N\";\n    pseudolbl_up       = 0.90;\n    pseudolbl_low      = 0.05;\n    \n    use_orig_allfolds  = \"N\";\n    n_splits           = 10 ;\n    n_repeats          = 1 ;\n    nbrnd_erly_stp     = 75 ;\n    mdlcv_mthd         = 'RKF';\n    \n    # Ensemble:-    \n    ensemble_req       = \"Y\";\n    hill_climb_req     = \"N\";\n    optuna_req         = \"N\";\n    LAD_req            = \"Y\";\n    enscv_mthd         = \"RKF\";\n    metric_obj         = 'minimize';\n    ntrials            = 10 if test_req == \"Y\" else 500;\n    \n    # Global variables for plotting:-\n    grid_specs = {'visible': True, 'which': 'both', 'linestyle': '--', \n                           'color': 'lightgrey', 'linewidth': 0.75};\n    title_specs = {'fontsize': 9, 'fontweight': 'bold', 'color': 'tab:blue'};\n\nprint();\nPrintColor(f\"--> Configuration done!\\n\");\ncollect();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:56.242938Z","iopub.execute_input":"2023-11-18T11:49:56.243389Z","iopub.status.idle":"2023-11-18T11:49:56.394378Z","shell.execute_reply.started":"2023-11-18T11:49:56.243338Z","shell.execute_reply":"2023-11-18T11:49:56.39313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\n# Defining functions to be used throughout the code for common tasks:-\n\n# Scaler to be used for continuous columns:- \nall_scalers = {'Robust': RobustScaler(), \n               'Z': StandardScaler(), \n               'MinMax': MinMaxScaler(),\n               \"PT\" : PowerTransformer(),\n              };\nscaler      = all_scalers.get(CFG.scl_method);\n\n# Commonly used CV strategies for later usage:-\nall_cv= {'KF'  : KFold(n_splits= CFG.n_splits, shuffle = True, random_state= CFG.state),\n         'RKF' : RKF(n_splits= CFG.n_splits, n_repeats = CFG.n_repeats, random_state= CFG.state),\n         'RSKF': RSKF(n_splits= CFG.n_splits, n_repeats = CFG.n_repeats, random_state= CFG.state),\n         'SKF' : SKF(n_splits= CFG.n_splits, shuffle = True, random_state= CFG.state)\n        };\n\n# Defining the competition metric:-\ndef ScoreMetric(ytrue, ypred, tgt_scl_fct = CFG.tgt_scl_fct)-> float:\n    \"\"\"\n    This function calculates the metric for the competition. \n    ytrue- ground truth array\n    ypred- predictions\n    returns - metric value (float)\n    \n    This function necessitates that the predictions array should be flattened.\n    \"\"\";\n    \n    return medae(ytrue, ypred);\n\ndef PostProcessPred(preds, post_process = \"N\", tgt_scl_fct = CFG.tgt_scl_fct):\n    \"\"\"\n    This is an optional post-processing function. \n    We correct predictions to be within the specific range\n    \"\"\";\n    if post_process == \"Y\":\n        preds_pp = np.clip(preds, a_min = 0, a_max= 10 * tgt_scl_fct);\n        return preds_pp;\n    else:\n        return preds;\n\n# Designing a custom scorer to use in cross_val_predict and cross_val_score:-\nmyscorer = make_scorer(ScoreMetric, \n                       greater_is_better = False if CFG.metric_obj == \"minimize\" else True, \n                       needs_proba= False,\n                      );\n\n# Making a directory to store the model objects:-\nif os.path.exists(\"/kaggle/working/MLModels\") == True:\n    PrintColor(f\"\\n---> MLModels already exists\\n\", color = Fore.RED);\nelse:\n    os.mkdir(\"MLModels\");\n    PrintColor(f\"\\n---> Made directory MLModels to store models for inference\\n\", color = Fore.RED)\n\n\ncollect();\nprint();\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:56.395828Z","iopub.execute_input":"2023-11-18T11:49:56.396174Z","iopub.status.idle":"2023-11-18T11:49:56.536006Z","shell.execute_reply.started":"2023-11-18T11:49:56.39612Z","shell.execute_reply":"2023-11-18T11:49:56.535213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.2\"></a>\n## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #0059b3; border-bottom: 8px solid #e6e6e6\" > CONFIGURATION PARAMETERS<br><div> \n\n\n| Parameter         | Description                                             | Possible value choices|\n| ---               | ---                                                     | :-:                   |\n|  version_nb       | Version Number                                          | integer               |\n|  test_req         | Are we testing syntax here?                             | Y/N                   |  \n|  test_sample_frac | Sample size for syntax test                             | float(0-1)/ int       |     \n|  gpu_switch       | GPU switch                                              | ON/OFF                |\n|  state            | Random state for most purposes                          | integer               |\n|  target           | Target column name                                      | yield                 |\n|  episode          | Episode Number                                          | integer               |\n|  path             | Path for input data files                               |                       |\n|  orig_path        | Path for input original data files                      |                       |\n|  dtl_preproc_req  | Proprocessing required                                  | Y/N                   |    \n|  adv_cv_req       | Adversarial CV required                                 | Y/N                   |\n|  ftre_plots_req   | Feature plots required                                  | Y/N                   |\n|  ftre_imp_req     | Feature importance required                             | Y/N                   |\n|  conjoin_orig_data| Conjoin original data                                   | Y/N                   |\n|  drop_nulls       | Drop original data nulls                                | Y/N                   |    \n|  sec_ftre_req     | Secondary features required                             | Y/N                   |\n|  scale_req        | Scaling required                                        | Y/N                   |\n|  scl_method       | Scaling method                                          | Z/ Robust/ MinMax     |\n|  enc_method       | Encoding method                                         |-                      |\n|  OH_cols          | Onehot columns                                          |list                   |\n|  tgt_scl_fct      | Target scaling factor                                   |integer                |\n|  tgt_mapper       | Target mapper                                           | dict                  |\n|  drop_tr_req      | Drop extra training elements not in test                | Y/N                   |\n|  baseline_req     | Baseline model required                                 | Y/N                   |\n|  pstprcs_oof      | Post-process OOF after model training                   | Y/N                   |\n|  pstprcs_train    | Post-process OOF during model training for dev-set      | Y/N                   |\n|  pstprcs_test     | Post-process test after training                        | Y/N                   |\n|  ML               | Machine Learning Models                                 | Y/N                   |\n|  use_orig_all_folds| Use original data in all folds                         | Y/N                   |\n|  n_splits         | Number of CV splits                                     | integer               |\n|  n_repeats        | Number of CV repeats                                    | integer               |\n|  nbrnd_erly_stp   | Number of early stopping rounds                         | integer               |\n|  mdl_cv_mthd      | Model CV method name                                    | RKF/ RSKF/ SKF/ KFold |","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.3\"></a>\n## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #0059b3; border-bottom: 8px solid #e6e6e6\" > DATASET AND COMPETITION DETAILS<br><div>\n    \n**Data columns**<br>\nThis is available in the original data description as below<br>\nhttps://www.kaggle.com/datasets/jocelyndumlao/prediction-of-mohs-hardness-with-machine-learning <br>\n<br>**Competition details and notebook objectives**<br>\n1. This is a regression(?) challenge to predict material hardness. **Median-absolute-error** is the metric for the challenge<br>\n2. In this starter notebook, we start the assignment with a detailed EDA, feature plots, interaction effects, adversarial CV analysis and develop starter models to initiate the challenge. We will also incorporate other opinions and approaches as we move along the challenge.<br>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:black; font-size:120%; text-align:left;padding:3.0px; background: #cceeff; border-bottom: 8px solid #004466\" > PREPROCESSING<br><div> ","metadata":{}},{"cell_type":"code","source":"%time \n\nclass Preprocessor():\n    \"\"\"\n    This class aims to do the below-\n    1. Read the datasets\n    2. In this case, process the original data\n    3. Check information and description\n    4. Check unique values and nulls\n    5. Collate starting features \n    6. Conjoin train-original data if requested based on Adversarial CV results\n    \"\"\";\n    \n    def __init__(self):\n        self.train             = pd.read_csv(os.path.join(CFG.path,\"train.csv\"), index_col = 'id');\n        self.test              = pd.read_csv(os.path.join(CFG.path ,\"test.csv\"), index_col = 'id');\n        self.target            = CFG.target ;\n        self.original          = pd.read_csv(CFG.orig_path);\n        self.conjoin_orig_data = CFG.conjoin_orig_data;\n        self.dtl_preproc_req   = CFG.dtl_preproc_req;\n        self.test_req          = CFG.test_req;\n        \n        self.sub_fl   = pd.read_csv(os.path.join(CFG.path, \"sample_submission.csv\"));\n        PrintColor(f\"Data shapes - train-test-original = {self.train.shape} {self.test.shape} {self.original.shape}\");\n        \n        for tbl in [self.train, self.original, self.test]:\n            tbl.columns = tbl.columns.str.replace(r\"\\(|\\)|\\s+\",\"\", regex = True);\n            tbl['allelectrons_Total'] = tbl['allelectrons_Total'].astype(np.uint16);\n        \n        PrintColor(f\"\\nTrain set head\", color = Fore.GREEN);\n        display(self.train.head(5).style.format(precision = 3));\n        PrintColor(f\"\\nTest set head\", color = Fore.GREEN);\n        display(self.test.head(5).style.format(precision = 3));\n        PrintColor(f\"\\nOriginal set head\", color = Fore.GREEN);\n        display(self.original.head(5).style.format(precision = 3));\n                 \n        # Resetting original data index:-\n        self.original.index = range(len(self.original));\n        self.original.index+= max(self.test.index) + 1;\n        self.original.index.name = 'id';\n        \n        #  Changing original data column order to match the competition column structure:-\n        self.original = self.original.reindex(self.train.columns, axis=1);\n              \n    def _AddSourceCol(self):\n        self.train['Source'] = \"Competition\";\n        self.test['Source']  = \"Competition\";\n        self.original['Source'] = 'Original';\n        \n        self.strt_ftre = self.test.columns;\n        return self;\n          \n    def _CollateInfoDesc(self):\n        if self.dtl_preproc_req == \"Y\":\n            PrintColor(f\"\\n{'-'*20} Information and description {'-'*20}\\n\", color = Fore.MAGENTA);\n\n            # Creating dataset information and description:\n            for lbl, df in {'Train': self.train, 'Test': self.test, 'Original': self.original}.items():\n                PrintColor(f\"\\n{lbl} description\\n\");\n                display(df.describe(percentiles= [0.05, 0.25, 0.50, 0.75, 0.9, 0.95, 0.99]).\\\n                        transpose().\\\n                        drop(columns = ['count'], errors = 'ignore').\\\n                        drop([CFG.target], axis=0, errors = 'ignore').\\\n                        style.format(formatter = '{:,.2f}').\\\n                        background_gradient(cmap = 'Blues')\n                       );\n\n                PrintColor(f\"\\n{lbl} information\\n\");\n                display(df.info());\n                collect();\n        return self;\n    \n    def _CollateUnqNull(self):\n        \n        if self.dtl_preproc_req == \"Y\":\n            # Dislaying the unique values across train-test-original:-\n            PrintColor(f\"\\nUnique and null values\\n\");\n            _ = pd.concat([self.train[self.strt_ftre].nunique(), \n                           self.test[self.strt_ftre].nunique(), \n                           self.original[self.strt_ftre].nunique(),\n                           self.train[self.strt_ftre].isna().sum(axis=0),\n                           self.test[self.strt_ftre].isna().sum(axis=0),\n                           self.original[self.strt_ftre].isna().sum(axis=0)\n                          ], \n                          axis=1);\n            _.columns = ['Train_Nunq', 'Test_Nunq', 'Original_Nunq', \n                         'Train_Nulls', 'Test_Nulls', 'Original_Nulls'\n                        ];\n\n            display(_.T.style.background_gradient(cmap = 'Blues', axis=1).\\\n                    format(formatter = '{:,.0f}')\n                   );\n            \n        return self;\n       \n    def DoPreprocessing(self):\n        self._AddSourceCol();\n        self._CollateInfoDesc();\n        self._CollateUnqNull();\n        \n        return self; \n        \n    def ConjoinTrainOrig(self):\n        if self.conjoin_orig_data == \"Y\":\n            PrintColor(f\"Train shape before conjoining with original = {self.train.shape}\");\n            train = pd.concat([self.train, self.original], axis=0, ignore_index = True);\n            PrintColor(f\"Train shape after conjoining with original= {train.shape}\");\n            \n            train = train.drop_duplicates();\n            PrintColor(f\"Train shape after de-duping = {train.shape}\");\n            \n            train.index = range(len(train));\n            train.index.name = 'id';\n        \n        else:\n            PrintColor(f\"We are using the competition training data only\");\n            train = self.train;\n        return train;\n          \ncollect();\nprint();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:56.537574Z","iopub.execute_input":"2023-11-18T11:49:56.538005Z","iopub.status.idle":"2023-11-18T11:49:56.696687Z","shell.execute_reply.started":"2023-11-18T11:49:56.537974Z","shell.execute_reply":"2023-11-18T11:49:56.695547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\npp = Preprocessor();\npp.DoPreprocessing();\n\nif CFG.ensemble_req == \"Y\":\n    sub_fl = pp.sub_fl.copy(deep = True);\n\nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:56.698399Z","iopub.execute_input":"2023-11-18T11:49:56.698714Z","iopub.status.idle":"2023-11-18T11:49:57.641822Z","shell.execute_reply.started":"2023-11-18T11:49:56.698687Z","shell.execute_reply":"2023-11-18T11:49:57.640796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #0059b3; border-bottom: 8px solid #e6e6e6\" > INFERENCES<br> <div>","metadata":{}},{"cell_type":"markdown","source":"<div style= \"font-family: Cambria; letter-spacing: 0px; color:#000000; font-size:110%; text-align:left;padding:3.0px; background: #f2f2f2\" >\n1. We have numerical columns<br>\n2. Original dataset has no nulls.<br>\n3. The dataset is quite small, so building a CV is difficult- luck will play a huge part in this challenge<br>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:black; font-size:120%; text-align:left;padding:3.0px; background: #cceeff; border-bottom: 8px solid #004466\" > ADVERSARIAL CV<br><div>","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Performing adversarial CV between the 2 specified datasets:-\ndef Do_AdvCV(df1:pd.DataFrame, df2:pd.DataFrame, source1:str, source2:str):\n    \"This function performs an adversarial CV between the 2 provided datasets if needed by the user\";\n    \n    # Adversarial CV per column:-\n    ftre = pp.test.select_dtypes(include = np.number).\\\n    drop(columns = ['id', \"Source\"], errors = 'ignore').columns;\n    adv_cv = {};\n\n    for col in ftre:\n        shuffle_state = np.random.randint(low = 10, high = 100, size= 1);\n\n        full_df = \\\n        pd.concat([df1[[col]].assign(Source = source1), df2[[col]].assign(Source = source2)], \n                  axis=0, ignore_index = True).\\\n        sample(frac = 1.00, random_state = shuffle_state);\n\n        full_df = full_df.assign(Source_Nb = full_df['Source'].eq(source2).astype(np.int8));\n\n        # Checking for adversarial CV:-\n        model = LGBMClassifier(random_state = CFG.state, max_depth = 6, learning_rate = 0.05);\n        cv    = all_cv['SKF'];\n        score = np.mean(cross_val_score(model, \n                                        full_df[[col]], \n                                        full_df.Source_Nb, \n                                        scoring= 'roc_auc', \n                                        cv     = cv)\n                       );\n        adv_cv.update({col: round(score, 4)});\n        collect();\n    \n    del ftre;\n    collect();\n    \n    fig, ax = plt.subplots(1,1,figsize = (12, 5));\n    pd.Series(adv_cv).plot.bar(color = 'tab:blue', ax = ax);\n    ax.axhline(y = 0.60, color = 'red', linewidth = 2.75);\n    ax.grid(**CFG.grid_specs); \n    plt.yticks(np.arange(0.0, 0.81, 0.05));\n    plt.show();\n    \n# Implementing the adversarial CV:-\nif CFG.adv_cv_req == \"Y\":\n    PrintColor(f\"\\n---------- Adversarial CV - Train vs Original ----------\\n\", \n               color = Fore.MAGENTA);\n    Do_AdvCV(df1 = pp.train, df2 = pp.original, source1 = 'Train', source2 = 'Original');\n    \n    PrintColor(f\"\\n---------- Adversarial CV - Train vs Test ----------\\n\", \n               color = Fore.MAGENTA);\n    Do_AdvCV(df1 = pp.train, df2 = pp.test, source1 = 'Train', source2 = 'Test');\n    \n    PrintColor(f\"\\n---------- Adversarial CV - Original vs Test ----------\\n\", \n               color = Fore.MAGENTA);\n    Do_AdvCV(df1 = pp.original, df2 = pp.test, source1 = 'Original', source2 = 'Test');   \n    \nif CFG.adv_cv_req == \"N\":\n    PrintColor(f\"\\nAdversarial CV is not needed\\n\", color = Fore.RED);\n    \ncollect();\nprint();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:57.643516Z","iopub.execute_input":"2023-11-18T11:49:57.644473Z","iopub.status.idle":"2023-11-18T11:49:57.786591Z","shell.execute_reply.started":"2023-11-18T11:49:57.644433Z","shell.execute_reply":"2023-11-18T11:49:57.785621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nprint();\ntrain, test, strt_ftre = pp.ConjoinTrainOrig(), pp.test.copy(deep = True), deepcopy(pp.strt_ftre);\ncat_cols  = [];\ncont_cols = [col for col in pp.strt_ftre if col not in cat_cols + [CFG.target, \"Source\"]];\n\nif CFG.drop_nulls == \"Y\":\n    train = train.dropna();\nelse: \n    pass;\n\nPrintColor(f\"\\nCategory columns\\n\");\ndisplay(cat_cols);\nPrintColor(f\"\\nContinuous columns\\n\");\ndisplay(np.array(cont_cols));\nPrintColor(f\"\\nAll columns\\n\");\ndisplay(strt_ftre);\n\nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:57.790315Z","iopub.execute_input":"2023-11-18T11:49:57.790739Z","iopub.status.idle":"2023-11-18T11:49:57.955978Z","shell.execute_reply.started":"2023-11-18T11:49:57.790709Z","shell.execute_reply":"2023-11-18T11:49:57.954637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #0059b3; border-bottom: 8px solid #e6e6e6\" > INFERENCES<br><div>","metadata":{}},{"cell_type":"markdown","source":"<div style= \"font-family: Cambria; letter-spacing: 0px; color:#000000; font-size:110%; text-align:left;padding:3.0px; background: #f2f2f2\" >\n1. Train-test belong to the same distribution, we can perhaps rely on the CV score<br>\n2. We need to further check the train-original distribution further, adversarial validation results indicate that we cannot use the original dataset based on a couple of features<br>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:black; font-size:120%; text-align:left;padding:3.0px; background: #cceeff; border-bottom: 8px solid #004466\" > VISUALS AND EDA <br><div> \n ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5.2\"></a>\n## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #0059b3; border-bottom: 8px solid #e6e6e6\" > TARGET PLOT<br><div>","metadata":{}},{"cell_type":"code","source":"%%time \n\nif CFG.ftre_plots_req == \"Y\":\n    fig, axes = plt.subplots(1,2, figsize = (20, 5), sharey = True, gridspec_kw = {'wspace': 0.35});\n    \n    for i, df in tqdm(enumerate([pp.train, pp.original]), \"Target plot ---> \"):\n        ax= axes[i];\n        a = df[CFG.target].value_counts(normalize = True);\n        a.sort_index().plot.bar(color = '#3377ff', ax = ax);\n        df_name = 'Train' if i == 0 else \"Original\";\n        _ = ax.set_title(f\"\\n{df_name} data\\n\", **CFG.title_specs);\n        \n    plt.tight_layout();\n    plt.show();\n    \nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:57.957186Z","iopub.execute_input":"2023-11-18T11:49:57.9575Z","iopub.status.idle":"2023-11-18T11:49:58.096528Z","shell.execute_reply.started":"2023-11-18T11:49:57.957474Z","shell.execute_reply":"2023-11-18T11:49:58.095382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.5\"></a>\n## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #0059b3; border-bottom: 8px solid #e6e6e6\" > CONTINUOUS COLUMN PLOTS<br><div>","metadata":{}},{"cell_type":"code","source":"%%time \n\nif CFG.ftre_plots_req == \"Y\":\n    df = pd.concat([pp.train[cont_cols].assign(Source = 'Train'), \n                    pp.test[cont_cols].assign(Source = 'Test'),\n                    pp.original[cont_cols].assign(Source = \"Original\")\n                   ], \n                   axis=0, ignore_index = True\n                  );\n    \n    fig, axes = plt.subplots(len(cont_cols), 4 ,figsize = (16, len(cont_cols) * 4.2), \n                             gridspec_kw = {'hspace': 0.35, 'wspace': 0.3, 'width_ratios': [0.80, 0.20, 0.20, 0.20]});\n    \n    for i,col in enumerate(cont_cols):\n        ax = axes[i,0];\n        sns.kdeplot(data = df[[col, 'Source']], x = col, hue = 'Source', \n                    palette = ['#0039e6', '#ff5500', '#00b300'], \n                    ax = ax, linewidth = 2.1\n                   );\n        ax.set_title(f\"\\n{col}\", **CFG.title_specs);\n        ax.grid(**CFG.grid_specs);\n        ax.set(xlabel = '', ylabel = '');\n        \n        ax = axes[i,1];\n        sns.boxplot(data = df.loc[df.Source == 'Train', [col]], y = col, width = 0.25,\n                    color = '#33ccff', saturation = 0.90, linewidth = 0.90, \n                    fliersize= 2.25,\n                    ax = ax);\n        ax.set(xlabel = '', ylabel = '');\n        ax.set_title(f\"Train\", **CFG.title_specs);\n        \n        ax = axes[i,2];\n        sns.boxplot(data = df.loc[df.Source == 'Test', [col]], y = col, width = 0.25, fliersize= 2.25,\n                    color = '#80ffff', saturation = 0.6, linewidth = 0.90, \n                    ax = ax); \n        ax.set(xlabel = '', ylabel = '');\n        ax.set_title(f\"Test\", **CFG.title_specs);\n        \n        ax = axes[i,3];\n        sns.boxplot(data = df.loc[df.Source == 'Original', [col]], y = col, width = 0.25, fliersize= 2.25,\n                    color = '#99ddff', saturation = 0.6, linewidth = 0.90, \n                    ax = ax); \n        ax.set(xlabel = '', ylabel = '');\n        ax.set_title(f\"Original\", **CFG.title_specs);\n              \n    plt.suptitle(f\"\\nDistribution analysis- continuous columns\\n\", **CFG.title_specs, \n                 y = 0.905, x = 0.50\n                );\n    plt.tight_layout();\n    plt.show();\n    \nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:58.097986Z","iopub.execute_input":"2023-11-18T11:49:58.098743Z","iopub.status.idle":"2023-11-18T11:49:58.24935Z","shell.execute_reply.started":"2023-11-18T11:49:58.098698Z","shell.execute_reply":"2023-11-18T11:49:58.248386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\n# Calculating skewness across columns:-\nskew_df = pd.DataFrame(index = strt_ftre[0:-1]);\nfor col, df in {\"Train\": pp.train, \"Test\": pp.test, \"Original\": pp.original}.items():\n    skew_df = \\\n    pd.concat([skew_df, \n               df.drop(columns = [CFG.target, \"Source\", \"id\"], errors = \"ignore\").skew()],\n              axis=1).rename({0: col}, axis=1);\n\nPrintColor(f\"\\nSkewness across independent features\\n\");\ndisplay(skew_df.transpose().style.format(precision = 2).background_gradient(\"Pastel2\"));\n\ncollect();\nprint();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:58.250666Z","iopub.execute_input":"2023-11-18T11:49:58.251066Z","iopub.status.idle":"2023-11-18T11:49:58.423787Z","shell.execute_reply.started":"2023-11-18T11:49:58.251034Z","shell.execute_reply":"2023-11-18T11:49:58.422695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.6\"></a>\n## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #0059b3; border-bottom: 8px solid #e6e6e6\" > CATEGORY COLUMN PLOTS<br><div>","metadata":{}},{"cell_type":"code","source":"%%time \n\nif CFG.ftre_plots_req == \"Y\" and cat_cols != []:\n    fig, axes = plt.subplots(len(cat_cols), 3, figsize = (20, len(cat_cols)* 4.5), \n                             gridspec_kw = {'wspace': 0.25, 'hspace': 0.3});\n\n    for i, col in enumerate(cat_cols):\n        ax = axes[i, 0];\n        a = pp.train[col].value_counts(normalize = True);\n        a.sort_index().plot.barh(ax = ax, color = '#007399');\n        ax.set_title(f\"{col}_Train\", **CFG.title_specs);\n        ax.set_xticks(np.arange(0.0, 0.9, 0.05), \n                      labels = np.round(np.arange(0.0, 0.9, 0.05),2), \n                      rotation = 90\n                     );\n        ax.set(xlabel = '', ylabel = '');\n        del a;\n\n        ax = axes[i, 1];\n        a = pp.test[col].value_counts(normalize = True);\n        a.sort_index().plot.barh(ax = ax, color = '#0088cc');\n        ax.set_title(f\"{col}_Test\", **CFG.title_specs);\n        ax.set_xticks(np.arange(0.0, 0.9, 0.05), \n                      labels = np.round(np.arange(0.0, 0.9, 0.05),2), \n                      rotation = 90\n                     );\n        ax.set(xlabel = '', ylabel = '');\n        del a;\n        \n        ax = axes[i, 2];\n        a = pp.original[col].value_counts(normalize = True);\n        a.sort_index().plot.barh(ax = ax, color = '#0047b3');\n        ax.set_title(f\"{col}_Original\", **CFG.title_specs);\n        ax.set_xticks(np.arange(0.0, 0.9, 0.05), \n                      labels = np.round(np.arange(0.0, 0.9, 0.05),2), \n                      rotation = 90\n                     );\n        ax.set(xlabel = '', ylabel = '');\n        del a;       \n    \n    plt.suptitle(f\"Category column plots\", **CFG.title_specs, y= 0.90);\n    plt.tight_layout();\n    plt.show();\n    \nprint();\ncollect();\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:58.42485Z","iopub.execute_input":"2023-11-18T11:49:58.425155Z","iopub.status.idle":"2023-11-18T11:49:58.563805Z","shell.execute_reply.started":"2023-11-18T11:49:58.425116Z","shell.execute_reply":"2023-11-18T11:49:58.563011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.7\"></a>\n## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #0059b3; border-bottom: 8px solid #e6e6e6\" > FEATURE INTERACTION AND UNIVARIATE RELATIONS<br><div>\n    \n","metadata":{}},{"cell_type":"code","source":"%%time \n\ndef MakeCorrPlot(df: pd.DataFrame, data_label:str, figsize = (30, 9)):\n    \"\"\"\n    This function develops the correlation plots for the given dataset\n    \"\"\";\n    \n    fig, axes = plt.subplots(1,2, figsize = figsize, gridspec_kw = {'hspace': 0.2, 'wspace': 0.1},\n                             sharey = True\n                            );\n    \n    for i, method in enumerate(['pearson', 'spearman']):\n        corr_ = df.drop(columns = ['id', 'Source'], errors = 'ignore').corr(method = method);\n        ax = axes[i];\n        sns.heatmap(data = corr_,  \n                    annot= True,\n                    fmt= '.2f', \n                    cmap = 'Blues',\n                    annot_kws= {'fontweight': 'bold','fontsize': 6.75}, \n                    linewidths= 1.5, \n                    linecolor='white', \n                    cbar= False, \n                    mask= np.triu(np.ones_like(corr_)),\n                    ax= ax\n                   );\n        ax.set_title(f\"\\n{method.capitalize()} correlation- {data_label}\\n\", **CFG.title_specs);\n        \n    collect();\n    print();\n\n# Implementing correlation analysis:-\nif CFG.ftre_imp_req == \"Y\":\n    for lbl, df in {\"Train\": pp.train, \n                    \"Test\": pp.test, \n                    \"Original\": pp.original\n                   }.items():\n        MakeCorrPlot(df = df, data_label = lbl, figsize = (22,6.5));\n\nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:58.565404Z","iopub.execute_input":"2023-11-18T11:49:58.565821Z","iopub.status.idle":"2023-11-18T11:49:58.710658Z","shell.execute_reply.started":"2023-11-18T11:49:58.565782Z","shell.execute_reply":"2023-11-18T11:49:58.709593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.12\"></a>\n## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #0059b3; border-bottom: 8px solid #e6e6e6\" > INFERENCES<br> <div>","metadata":{}},{"cell_type":"markdown","source":"<div style= \"font-family: Cambria; letter-spacing: 0px; color:#000000; font-size:110%; text-align:left;padding:3.0px; background: #f2f2f2\" >\n1. Features are positively and negatively skewed and are all non-negative. Feature transforms may be useful for non-tree models<br>\n2. Outlier treatment could be considered as an option considering outlier presence across all columns in the data<br>\n3. Higher correlation between quite a few columns may necessitate methods like PCA <br>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:black; font-size:120%; text-align:left;padding:3.0px; background: #cceeff; border-bottom: 8px solid #004466\" > DATA TRANSFORMS <br><div> \n    \nThis section aims at creating secondary features, scaling and if necessary, conjoining the competition training and original data tables<br>\n","metadata":{}},{"cell_type":"code","source":"%%time \n\n# Data transforms:-\nclass Xformer(TransformerMixin, BaseEstimator):\n    \"\"\"\n    This class adds secondary features to the existing data using simple interactions\n    \"\"\";\n    \n    def __init__(self): \n        pass\n    \n    def fit(self, X, y= None, **params):\n        return self;\n    \n    @staticmethod\n    def _reduce_mem(df: pd.DataFrame):\n        \"This method reduces memory for numeric columns in the dataframe\";\n        \n        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n        start_mem = df.memory_usage().sum() / 1024**2;\n        \n        for col in df.columns:\n            col_type = df[col].dtypes\n            \n            if col_type in numerics:\n                c_min = df[col].min();\n                c_max = df[col].max();\n\n                if \"int\" in str(col_type)[:3]:\n                    if c_min >= np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min >= np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min >= np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min >= np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min >= np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    if c_min >= np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)  \n\n        end_mem = df.memory_usage().sum() / 1024**2\n    \n        PrintColor(f\"Start - end memory:- {start_mem:5.2f} - {end_mem:5.2f} Mb\");\n        return df;\n    \n    def transform(self, X, y= None, **params):\n        \"\"\"\n        This method adds secondary features to the existing data\n        \"\"\";\n        \n        df    = X.copy(); \n        self.op_cols = df.columns;\n        df = self._reduce_mem(df);\n        return df;\n    \n    def get_feature_names_in(self, X, y=None, **params): \n        return self.ip_cols;    \n    \n    def get_feature_names_out(self, X, y=None, **params): \n        return self.op_cols;\n    \ncollect();\nprint();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:58.712321Z","iopub.execute_input":"2023-11-18T11:49:58.712751Z","iopub.status.idle":"2023-11-18T11:49:58.857084Z","shell.execute_reply.started":"2023-11-18T11:49:58.712718Z","shell.execute_reply":"2023-11-18T11:49:58.855863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\n# Scaling:-\nclass Scaler(TransformerMixin, BaseEstimator):\n    \"\"\"\n    This class aims to create scaling for the provided dataset\n    \"\"\";\n    \n    def __init__(self, scl_method: str, scale_req: str, scl_cols):\n        self.scl_method = scl_method;\n        self.scale_req  = scale_req;\n        self.scl_cols   = scl_cols;\n        \n    def fit(self,X, y=None, **params):\n        \"This function calculates the train-set parameters for scaling\";\n        \n        self.params          = X[self.scl_cols].describe(percentiles = [0.25, 0.50, 0.75]).drop(['count'], axis=0).T;\n        self.params['iqr']   = self.params['75%'] - self.params['25%'];\n        self.params['range'] = self.params['max'] - self.params['min'];\n        \n        return self;\n    \n    def transform(self,X, y=None, **params):  \n        \"This function transform the relevant scaling columns\";\n        \n        df = X.copy();\n        if self.scale_req == \"Y\":\n            if CFG.scl_method == \"Z\":\n                df[self.scl_cols] = (df[self.scl_cols].values - self.params['mean'].values) / self.params['std'].values;\n            elif CFG.scl_method == \"Robust\":\n                df[self.scl_cols] = (df[self.scl_cols].values - self.params['50%'].values) / self.params['iqr'].values;\n            elif CFG.scl_method == \"MinMax\":\n                df[self.scl_cols] = (df[self.scl_cols].values - self.params['min'].values) / self.params['range'].values;\n        else:\n            PrintColor(f\"Scaling is not needed\", color = Fore.RED);\n    \n        return df;\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:58.8583Z","iopub.execute_input":"2023-11-18T11:49:58.85869Z","iopub.status.idle":"2023-11-18T11:49:58.872568Z","shell.execute_reply.started":"2023-11-18T11:49:58.858658Z","shell.execute_reply":"2023-11-18T11:49:58.871395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nPrintColor(f\"\\n{'='* 20} Data transformation {'='* 20} \\n\");\n\nytrain = train[CFG.target].astype(np.float32);\nmapper = {209: 200, 220: 229, 230: 229, 260: 250, 410: 400, 469: 480, \n          540: 550, 575: 580, 640: 650, 664: 650, 670: 680, 810: 800, 830: 850, \n          980: 1000\n         };\n\nif CFG.tgt_scl_fct > 1:\n    ytrain = (ytrain * CFG.tgt_scl_fct).astype(np.uint16);\n    # Cleaning the targets:-\n    for k, v in mapper.items(): \n        ytrain.loc[ytrain == k] = v;\n    \nXtrain = train.drop(columns = [CFG.target]);\nXtest  = test.copy(deep = True);\n\nPrintColor(f\"\\n---> Train data\\n\");\ndisplay(Xtrain.head(5).style.format(precision = 2));\nPrintColor(f\"\\n---> Test data\\n\");\ndisplay(Xtest.head(5).style.format(precision = 2));\n\nprint();\nxform  = Pipeline(steps = [(\"S1\", Xformer()),\n                           (\"S2\", ColumnTransformer([(\"LN\", FT(np.log1p), ['density_Total','allelectrons_Average',\n                                                                           'atomicweight_Average','density_Average'\n                                                                          ]\n                                                     ),\n                                                     (\"SQ\", FT(np.square), ['val_e_Average','ionenergy_Average',\n                                                                            'el_neg_chi_Average','R_vdw_element_Average',\n                                                                            'R_cov_element_Average','zaratio_Average'])\n                                                    ], remainder= \"passthrough\", verbose_feature_names_out = False,\n                                                   )\n                           ),\n                          ]\n                 );\nXtrain = xform.fit_transform(Xtrain, ytrain);\nXtest  = xform.transform(Xtest);\nprint();\n\nPrintColor(f\"\\n---> Train data columns after data pipeline\\n\");\npprint(Xtrain.columns);\n\nPrintColor(f\"\\n---> Test data columns after data pipeline\\n\");\npprint(Xtest.columns);\nPrintColor(f\"\\n---> Train-test shape after pipeline = {Xtrain.shape} {Xtest.shape}\");\n\nPrintColor(f\"\\n---> Dropping duplicates in the train data\");\n_ = pd.concat([Xtrain, ytrain], axis=1).drop_duplicates(subset = Xtrain.columns[0:-1].to_list());\n_.index = range(len(_));\nXtrain, ytrain = _.drop(columns = [CFG.target]), _[CFG.target];\ndel _;\nPrintColor(f\"---> Train-test shape after duplicate removal = {Xtrain.shape} {Xtest.shape}\");\n\nprint();\ncollect();\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:58.874091Z","iopub.execute_input":"2023-11-18T11:49:58.874649Z","iopub.status.idle":"2023-11-18T11:49:59.091982Z","shell.execute_reply.started":"2023-11-18T11:49:58.874617Z","shell.execute_reply":"2023-11-18T11:49:59.090797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\n# Shortlisting the duplicate data between train and test dataframes for post-processing:-\njoin_cols = Xtrain.columns[0:-1].to_list();\n\ndups_df = \\\nXtrain.\\\nreset_index(names = \"id\").\\\nassign(Label = \"Train\").\\\nmerge(Xtest.reset_index().assign(Label = \"Test\"),\n      how = \"inner\", on = join_cols, suffixes = (\"_train\", \"_test\")).\\\ndrop(columns = join_cols).\\\nmerge(ytrain, how = \"inner\", left_on = \"id_train\", right_index = True)\\\n[['id_train', 'id_test', CFG.target]].\\\ndrop_duplicates(subset = ['id_test']);\n\ndel join_cols;\n\nPrintColor(f\"\\n---> Train-test duplicate data glimpse- shape = {dups_df.shape}\");\ndisplay(dups_df.head(10).style.format(precision = 3));\ndups_df.to_csv(f\"Duplicates.csv\", index = None);\n\ncollect();\nprint();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:59.093516Z","iopub.execute_input":"2023-11-18T11:49:59.093944Z","iopub.status.idle":"2023-11-18T11:49:59.268027Z","shell.execute_reply.started":"2023-11-18T11:49:59.093905Z","shell.execute_reply":"2023-11-18T11:49:59.266889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\n# Reducing to small sample for training as part of syntax testing:-\nif CFG.test_req == \"Y\":\n    PrintColor(f\"\\n---> Testing syntax and reducing data size\");\n    \n    if isinstance(CFG.test_sample_frac, float) == True:\n        Xtrain = \\\n        pd.concat([Xtrain, ytrain], axis=1).\\\n        groupby([CFG.target]).\\\n        sample(frac = CFG.test_sample_frac);\n        \n       \n    elif isinstance(CFG.test_sample_frac, int) == True:\n        Xtrain = \\\n        pd.concat([Xtrain, ytrain], axis=1).\\\n        groupby([CFG.target]).\\\n        sample(n = CFG.test_sample_frac);\n        \n    ytrain = ytrain.loc[ytrain.index.isin(Xtrain.index)];\n    Xtrain.index = range(len(Xtrain));\n    ytrain.index = range(len(ytrain)); \n    \nelse:\n    PrintColor(f\"\\n---> We are not testing syntax here\\n\");\n    \nPrintColor(f\"---> Train-test shape after syntax check = {Xtrain.shape} {ytrain.shape} {Xtest.shape}\");\n    \nprint();\ncollect();        ","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:59.269103Z","iopub.execute_input":"2023-11-18T11:49:59.269452Z","iopub.status.idle":"2023-11-18T11:49:59.406312Z","shell.execute_reply.started":"2023-11-18T11:49:59.269423Z","shell.execute_reply":"2023-11-18T11:49:59.404963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:black; font-size:120%; text-align:left;padding:3.0px; background: #cceeff; border-bottom: 8px solid #004466\" > MODEL TRAINING <br><div> \n   ","metadata":{}},{"cell_type":"code","source":"%%time \n\n# Initializing model I-O:-\n\nMdl_Master = \\\n{'CB1R': CatBoostRegressor(**{ 'task_type'          : \"GPU\" if CFG.gpu_switch == \"ON\" else \"CPU\",\n                              'objective'           : 'Quantile:alpha=0.50',\n                              'loss_function'       : 'Quantile:alpha=0.50',\n                              'eval_metric'         : 'Quantile:alpha=0.50',\n                              'bagging_temperature' : 0.1,\n                              'colsample_bylevel'   : 0.88,\n                              'iterations'          : 1000,\n                              'learning_rate'       : 0.035,\n                              'od_wait'             : 12,\n                              'max_depth'           : 7,\n                              'l2_leaf_reg'         : 1.75,\n                              'min_data_in_leaf'    : 20,\n                              'random_strength'     : 0.1, \n                              'max_bin'             : 100,\n                              'verbose'             : 0,\n                              'use_best_model'      : True,\n                           }\n                         ),\n \n 'CB2R': CatBoostRegressor(**{'task_type'           : \"GPU\" if CFG.gpu_switch == \"ON\" else \"CPU\",\n                              'objective'           : 'Quantile:alpha=0.50',\n                              'loss_function'       : 'Quantile:alpha=0.50',\n                              'eval_metric'         : 'Quantile:alpha=0.50',\n                              'bagging_temperature' : 0.75,\n                              'colsample_bylevel'   : 0.35,\n                              'iterations'          : 1000,\n                              'learning_rate'       : 0.025,\n                              'od_wait'             : 23,\n                              'max_depth'           : 6,\n                              'l2_leaf_reg'         : 2.5,\n                              'min_data_in_leaf'    : 16,\n                              'random_strength'     : 0.1, \n                              'max_bin'             : 140,\n                              'verbose'             : 0,\n                              'use_best_model'      : True,\n                           }\n                         ),\n \n  'CB3R': CatBoostRegressor(**{'task_type'           : \"GPU\" if CFG.gpu_switch == \"ON\" else \"CPU\",\n                              'objective'           : 'Quantile:alpha=0.50',\n                              'loss_function'       : 'Quantile:alpha=0.50',\n                              'eval_metric'         : 'Quantile:alpha=0.50',\n                              'bagging_temperature' : 0.35,\n                              'colsample_bylevel'   : 0.5,\n                              'iterations'          : 1000,\n                              'learning_rate'       : 0.04,\n                              'od_wait'             : 30,\n                              'max_depth'           : 5,\n                              'l2_leaf_reg'         : 1.35,\n                              'min_data_in_leaf'    : 24,\n                              'random_strength'     : 0.5, \n                              'max_bin'             : 80,\n                              'verbose'             : 0,\n                              'use_best_model'      : True,\n                           }\n                         ), \n\n  'XGB1R': XGBRegressor(**{'tree_method'        : \"gpu_hist\" if CFG.gpu_switch == \"ON\" else \"hist\",\n                           'objective'          : 'reg:quantileerror',\n                           'quantile_alpha'     : 0.50,\n                           'random_state'       : CFG.state,\n                           'colsample_bytree'   : 0.7,\n                           'learning_rate'      : 0.02,\n                           'max_depth'          : 9,\n                           'n_estimators'       : 1100,                         \n                           'reg_alpha'          : 0.0035,\n                           'reg_lambda'         : 0.0012,\n                           'min_child_weight'   : 16,\n                           'early_stopping_rounds' : CFG.nbrnd_erly_stp,\n                        }\n                       ),\n \n  'XGB2R': XGBRegressor(**{'tree_method'         : \"gpu_hist\" if CFG.gpu_switch == \"ON\" else \"hist\",\n                            'objective'          : 'reg:quantileerror',\n                            'quantile_alpha'     : 0.50,\n                            'random_state'       : CFG.state,\n                            'colsample_bytree'   : 0.25,\n                            'learning_rate'      : 0.08,\n                            'max_depth'          : 8,\n                            'n_estimators'       : 1100,                         \n                            'reg_alpha'          : 0.1,\n                            'reg_lambda'         : 0.0001,\n                            'min_child_weight'   : 25,\n                            'early_stopping_rounds' : CFG.nbrnd_erly_stp,\n                           }\n                        ),\n \n   'XGB3R': XGBRegressor(**{'tree_method'        : \"gpu_hist\" if CFG.gpu_switch == \"ON\" else \"hist\",\n                            'objective'          : 'reg:quantileerror',\n                            'quantile_alpha'     : 0.50,\n                            'random_state'       : CFG.state,\n                            'colsample_bytree'   : 0.50,\n                            'learning_rate'      : 0.025,\n                            'max_depth'          : 6,\n                            'n_estimators'       : 1100,                         \n                            'reg_alpha'          : 0.01,\n                            'reg_lambda'         : 0.01,\n                            'min_child_weight'   : 21,\n                            'early_stopping_rounds' : CFG.nbrnd_erly_stp,\n                           }\n                        ), \n\n   \"HGBR\":  HGBR(loss              = 'quantile',\n                 quantile          = 0.50,\n                 learning_rate     = 0.07,\n                 early_stopping    = True,\n                 max_iter          = 200,\n                 max_depth         = 5,\n                 min_samples_leaf  = 23,\n                 l2_regularization = 1.75,\n                 scoring           = myscorer,\n                 random_state      = CFG.state,\n                ),\n};\n\nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:59.408136Z","iopub.execute_input":"2023-11-18T11:49:59.40846Z","iopub.status.idle":"2023-11-18T11:49:59.55887Z","shell.execute_reply.started":"2023-11-18T11:49:59.408433Z","shell.execute_reply":"2023-11-18T11:49:59.55788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nif CFG.ML == \"Y\":\n\n    # Initializing output tables for the models:-\n    if CFG.test_req == \"Y\" or CFG.pseudo_lbl_req == \"Y\":\n        methods = [\"CB1R\", \"CB2R\", \"XGB1R\", 'XGB2R'];\n    else:\n        methods = list(Mdl_Master.keys());\n  \n    sel_cols  = Xtrain.columns.to_list();\n    cat_ftre  = cat_cols;\n    scl_ftre  = cont_cols;\n    \n    if CFG.mdlcv_mthd in (\"SKF\", \"RSKF\"):\n        y_grp = np.uint8(ytrain / CFG.tgt_scl_fct);\n    else:\n        y_grp = ytrain.values;\n    \n    OOF_Preds = pd.DataFrame(columns = methods);\n    Mdl_Preds = pd.DataFrame(index = pp.sub_fl['id'], columns = methods);\n    FtreImp   = pd.DataFrame(index = sel_cols[0:-1], columns = methods);\n    Scores    = pd.DataFrame(columns = methods);\n\n    PrintColor(f\"\\n---> Selected model options- \");\n    pprint(methods, depth = 1, width = 100, indent = 5);\n    \n    PrintColor(f\"\\n---> Selected category columns- \");\n    pprint(np.array(cat_ftre), depth = 1, width = 100, indent = 5);\n    \n    PrintColor(f\"\\n---> Selected transformation columns- \");\n    pprint(np.array(scl_ftre), depth = 1, width = 100, indent = 5);\n\nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:59.560001Z","iopub.execute_input":"2023-11-18T11:49:59.560316Z","iopub.status.idle":"2023-11-18T11:49:59.705037Z","shell.execute_reply.started":"2023-11-18T11:49:59.560288Z","shell.execute_reply":"2023-11-18T11:49:59.703698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\ndef TrainMdl(method:str):\n    \n    global Mdl_Master, Mdl_Preds, OOF_Preds, \\\n    all_cv, FtreImp, Xtrain, ytrain, y_grp, \\\n    sel_cols, risk_ftre, scl_ftre ; \n    \n    if method.startswith(\"KNN\") or method.startswith(\"RIDGE\") or \\\n    method.startswith(\"LT\") == True or method.startswith(\"SV\") == True:\n        c_xform = \\\n        ColumnTransformer([(\"CXform\", all_scalers[CFG.scl_method], scl_ftre)\n                          ], remainder = 'passthrough', verbose_feature_names_out = False\n                         ); \n        model = Pipeline(steps = [(\"T\", c_xform), (\"M\", Mdl_Master.get(method))]); \n         \n    else: \n        model = Pipeline(steps = [(\"M\", Mdl_Master.get(method))]); \n    \n    cv        = all_cv.get(CFG.mdlcv_mthd);\n    Xt        = Xtest[sel_cols];\n    \n    if CFG.use_orig_allfolds == \"Y\":\n        X    = Xtrain[sel_cols].query(\"Source == 'Competition'\");\n        y    = ytrain.loc[ytrain.index.isin(X.index)]; \n        Orig = pd.concat([Xtrain, ytrain], axis=1).query(\"Source == 'Original'\")[sel_cols];\n        \n    elif CFG.use_orig_allfolds != \"Y\":\n        X,y = Xtrain[sel_cols], ytrain.copy(deep = True);\n                  \n    # Initializing I-O for the given seed:-        \n    test_preds = 0;\n    oof_preds  = pd.DataFrame(); \n    scores     = [];\n    ftreimp    = 0;\n    cols_drop  = ['Source', \"id\"];\n       \n    for fold_nb, (train_idx, dev_idx) in enumerate(cv.split(X, y_grp)):\n        Xtr  = X.iloc[train_idx].drop(columns = cols_drop, errors = 'ignore');  \n        Xdev = X.iloc[dev_idx].loc[X.Source == \"Competition\"].\\\n        drop(columns = cols_drop, errors = 'ignore'); \n        ytr  = y.loc[y.index.isin(Xtr.index)];\n        ydev = y.loc[y.index.isin(Xdev.index)];\n        \n        if CFG.test_req == \"Y\":\n            print(f\"---> {Xtr.shape} {Xdev.shape} {ytr.shape} {ydev.shape} {Xt.shape}\");\n        else:\n            pass;\n\n        if CFG.use_orig_allfolds == \"Y\":\n            Xtr = pd.concat([Xtr, Orig.drop(columns = [CFG.target, 'Source'], errors = 'ignore')], \n                            axis = 0, ignore_index = True);\n            ytr = pd.concat([ytr, Orig[CFG.target]], axis = 0, ignore_index = True);\n            \n        # Fitting the model:-          \n        if \"CB\" in method:    \n            model.fit(Xtr, ytr, \n                      M__eval_set = [(Xdev, ydev)], \n                      M__verbose = 0,\n                      M__early_stopping_rounds = CFG.nbrnd_erly_stp,\n                      M__cat_features= ['allelectrons_Total'],\n                     ); \n        \n        elif \"LGBM\" in method:\n            model.fit(Xtr, ytr, \n                      M__eval_set = [(Xdev, ydev)], \n                      M__callbacks = [log_evaluation(0), \n                                      early_stopping(stopping_rounds = CFG.nbrnd_erly_stp, \n                                                     verbose = False,),\n                                     ],\n                     ); \n            \n        elif \"XGB\" in method:\n             model.fit(Xtr, ytr, \n                       M__eval_set = [(Xdev, ydev)], \n                       M__verbose = 0,\n                      );            \n            \n        else: \n            model.fit(Xtr, ytr,); \n                 \n        # Collecting predictions and scores and post-processing OOF based on model method:-\n        dev_preds   = PostProcessPred(model.predict(Xdev), post_process= CFG.pstprcs_train);\n        test_preds  = test_preds + \\\n        PostProcessPred(model.predict(Xt.drop(columns = cols_drop, errors = 'ignore')),\n                        post_process= CFG.pstprcs_train);\n        train_preds = PostProcessPred(model.predict(Xtr), post_process= CFG.pstprcs_train);\n          \n        tr_score = ScoreMetric(ytr.values.flatten(), train_preds);\n        score = ScoreMetric(ydev.values.flatten(), dev_preds);\n        scores.append(score); \n        \n        Scores.loc[fold_nb, method] = np.round(score, decimals= 6);\n        oof_preds = pd.concat([oof_preds,\n                               pd.DataFrame(index   = Xdev.index, \n                                            data    = dev_preds,\n                                            columns = [method])\n                              ],axis=0, ignore_index= False\n                             ); \n        \n        num_space = 3 if fold_nb <= 9 else 2;\n        PrintColor(f\"Fold{fold_nb} {' ' * num_space} OOF = {score:.5f} | Train = {tr_score:.5f}| {method}\", \n                   color = Fore.GREEN);\n        del num_space;\n    \n        oof_preds = pd.DataFrame(oof_preds.groupby(level = 0)[method].mean());\n        oof_preds.columns = [method];\n        \n        try: \n            ftreimp += model[\"M\"].feature_importances_;\n        except: \n            ftreimp = 0;\n            \n    num_space = 10 - len(method);\n    PrintColor(f\"--> {method}{'-' * num_space} CV = {np.mean(scores):.6f}\\n\");\n    del num_space;\n    \n    OOF_Preds[f'{method}'] = PostProcessPred(oof_preds.values.flatten(), CFG.pstprcs_train);\n    \n    if CFG.mdlcv_mthd in ['KF', 'SKF']:\n        Mdl_Preds[f'{method}'] = test_preds.flatten()/ CFG.n_splits; \n        FtreImp[method]        = ftreimp / CFG.n_splits;\n    else:\n        Mdl_Preds[f'{method}'] = test_preds.flatten()/ (CFG.n_splits * CFG.n_repeats); \n        FtreImp[method]        = ftreimp / (CFG.n_splits * CFG.n_repeats);\n    \n    collect(); \n    \nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:59.706827Z","iopub.execute_input":"2023-11-18T11:49:59.707214Z","iopub.status.idle":"2023-11-18T11:49:59.860081Z","shell.execute_reply.started":"2023-11-18T11:49:59.707172Z","shell.execute_reply":"2023-11-18T11:49:59.858876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\ndef MakePseudoLbl(Mdl_Preds: pd.DataFrame, ytrain: pd.Series, \n                  Xtrain: pd.DataFrame, Xtest: pd.DataFrame, \n                  up_cutoff: float, low_cutoff: float,\n                  **kwargs\n                 ):\n    \"This function combines the confident test-set predictions to the train set for 2nd stage pseudo labels\";\n    \n    test_preds = np.mean(Mdl_Preds, axis=1).to_frame();\n    test_preds.rename(columns = {0: CFG.target}, inplace = True);\n    test_preds[CFG.target] = np.select([test_preds[CFG.target].values >= 0.90, \n                                        test_preds[CFG.target].values <= 0.05\n                                       ], [1, 0], -1\n                                      );\n    test_preds = test_preds.loc[test_preds[CFG.target] >= 0];\n    \n    display(test_preds.head(10));\n    \n    PrintColor(f\"Rows to be added to the training data = {test_preds.shape[0]}\");\n    \n    test_preds = pd.concat([Xtest, test_preds], axis=1, join= \"inner\");\n    \n    Xtrain = pd.concat([Xtrain, test_preds[Xtrain.columns]], axis=0, ignore_index = True);\n    Xtrain.index = range(len(Xtrain));\n    ytrain = pd.concat([ytrain, test_preds[CFG.target].astype(np.uint8)], \n                       axis=0, ignore_index = True\n                      );\n    ytrain.index = range(len(ytrain));\n    \n    PrintColor(f\"Training data shape for 2nd stage pseudo labels = {Xtrain.shape} {ytrain.shape}\");\n    \n    return Xtrain, ytrain;","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:59.861441Z","iopub.execute_input":"2023-11-18T11:49:59.861866Z","iopub.status.idle":"2023-11-18T11:49:59.872403Z","shell.execute_reply.started":"2023-11-18T11:49:59.861838Z","shell.execute_reply":"2023-11-18T11:49:59.871325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nif CFG.pseudo_lbl_req == \"Y\" and CFG.ML == \"Y\":\n    # Implementing the ML models for 1st stage pseudo labels:-\n    PrintColor(f\"\\n{'-' * 25} ML model training- Stage1 Pseudo Labels {'-' * 25}\\n\", \n               color = Fore.MAGENTA\n              );\n    for method in tqdm(methods, \"ML models----\"): \n        TrainMdl(method);\n   \n    PrintColor(f\"\\n{'-' * 20} OOF CV scores across methods- Stage1 Pseudo Labels {'-' * 20}\\n\", \n                   color = Fore.MAGENTA);\n    display(pd.concat([Scores.mean(axis = 0), Scores.std(axis = 0)], axis=1).\\\n            rename(columns = {0: 'Mean', 1: 'Std'}).T.\\\n            style.format(precision = 6).\\\n            background_gradient(subset = methods, cmap = 'cubehelix', axis=1)\n           );  \n    \n    # Combining confident test-set predictions to the train-set:- \n    Xtrain, ytrain = MakePseudoLbl(**{'Mdl_Preds': Mdl_Preds, \n                                      'ytrain'   : ytrain, \n                                      'Xtrain'   : Xtrain, \n                                      'Xtest'    : Xtest, \n                                      'up_cutoff': CFG.pseudolbl_up,\n                                      'low_cutoff': CFG.pseudolbl_low,\n                                     }\n                                  );\n        \n    # Initializig I-O for second stage pseudo labels:-\n    if CFG.test_req == \"Y\":\n        methods = [\"LGBM1R\", \"XGB1R\", \"LGBM2R\"];\n    else:\n        methods = list(Mdl_Master.keys());\n        \n    OOF_Preds = pd.DataFrame(columns = methods);\n    Mdl_Preds = pd.DataFrame(index = pp.sub_fl['id'], columns = methods);\n    FtreImp   = pd.DataFrame(index = sel_cols[1:], columns = methods);\n    Scores    = pd.DataFrame(columns = methods);\n    \n    PrintColor(f\"\\n---> Selected model options- \");\n    pprint(methods, depth = 1, width = 100, indent = 5);\n    \n    PrintColor(f\"\\n{'-' * 25} ML model training- Stage2 Pseudo Labels {'-' * 25}\\n\", \n               color = Fore.MAGENTA\n              );\n    for method in tqdm(methods, \"ML models----\"): TrainMdl(method);\n    \n    PrintColor(f\"\\n{'-' * 20} OOF CV scores across methods- Stage2 Pseudo Labels {'-' * 20}\\n\", \n                   color = Fore.MAGENTA);\n    display(pd.concat([Scores.mean(axis = 0), Scores.std(axis = 0)], axis=1).\\\n            rename(columns = {0: 'Mean', 1: 'Std'}).T.\\\n            style.format(precision = 6).\\\n            background_gradient(subset = methods, cmap = 'cubehelix', axis=1)\n           ); \n    \nelif CFG.ML == \"Y\":\n    PrintColor(f\"\\n{'-' * 25} ML model training {'-' * 25}\\n\", color = Fore.MAGENTA);\n    for method in tqdm(methods, \"ML models----\"): \n        TrainMdl(method);\n\n    PrintColor(f\"\\n{'-' * 20} OOF CV scores across methods {'-' * 20}\\n\", \n               color = Fore.MAGENTA);\n    display(pd.concat([Scores.mean(axis = 0), Scores.std(axis = 0)], axis=1).\\\n            rename(columns = {0: 'Mean', 1: 'Std'}).T.\\\n            style.format(precision = 6).\\\n            background_gradient(subset = methods, cmap = 'cubehelix', axis=1)\n           );    \nelse:\n    PrintColor(f\"\\nML models are not needed\\n\", color = Fore.RED);    \n\ncollect();\nprint();    ","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:49:59.873676Z","iopub.execute_input":"2023-11-18T11:49:59.874004Z","iopub.status.idle":"2023-11-18T11:54:40.789196Z","shell.execute_reply.started":"2023-11-18T11:49:59.873978Z","shell.execute_reply":"2023-11-18T11:54:40.788254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n   ","metadata":{}},{"cell_type":"code","source":"%%time \n\n# Analysing the model results and feature importances:-\nif CFG.ML == \"Y\":\n    fig, axes = plt.subplots(len(methods), 1, figsize = (25, len(methods) * 6),\n                             gridspec_kw = {'hspace': 0.2, 'wspace': 0.2},\n                            );\n    \n    for i, col in enumerate(methods):\n        try: \n            ax = axes[i];\n        except: \n            ax = axes[0];\n        FtreImp[col].plot.barh(ax = ax, color = '#0073e6');\n        ax.set_title(f\"{col} Importances\", **CFG.title_specs);\n        ax.set(xlabel = '', ylabel = '');\n        \n    plt.suptitle(f\"ML Model prediction feature importance\", y = 0.9125, **CFG.title_specs);\n    plt.tight_layout();\n    plt.show();  \n\ncollect();\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:54:40.790577Z","iopub.execute_input":"2023-11-18T11:54:40.790901Z","iopub.status.idle":"2023-11-18T11:54:43.564198Z","shell.execute_reply.started":"2023-11-18T11:54:40.790874Z","shell.execute_reply":"2023-11-18T11:54:43.56339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #0059b3; border-bottom: 8px solid #e6e6e6\" > INFERENCES<br> <div>","metadata":{}},{"cell_type":"markdown","source":"<div style= \"font-family: Cambria; letter-spacing: 0px; color:#000000; font-size:110%; text-align:left;padding:3.0px; background: #f2f2f2\" >\n1. We have to drop some features/ add secondary features to ensue better CV results. Train-OOF scores are very stable. <br>\n2. We need to fine-tune some model parameters too<br>\n3. We could try other model methods too, perhaps some simpler ones will be good enough <br>\n4. Custom objective/ metric class will also work in this case <br>\n5. We can explore pseudo labels also <br>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"8\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:black; font-size:120%; text-align:left;padding:3.0px; background: #cceeff; border-bottom: 8px solid #004466\" > ENSEMBLE AND SUBMISSION<br> <div> ","metadata":{}},{"cell_type":"code","source":"%%time \n\nif CFG.ensemble_req == \"Y\" and CFG.optuna_req == \"Y\":\n    def Objective(trial):\n        \"This function defines the objective for the optuna ensemble using variable models\";\n\n        global OOF_Preds, all_cv, ytrain, methods, y_grp;\n\n        # Define the weights for the predictions from each model:-\n        weights  = [trial.suggest_float(f\"M{n}\", 0.0001, 0.9999, step = 0.001) \\\n                    for n in range(len(OOF_Preds[methods].columns))\n                   ];\n\n        # Calculating the CV-score for the weighted predictions on the competition data only:-\n        scores = [];  \n        cv     = all_cv[CFG.enscv_mthd];\n        X,y    = OOF_Preds[methods], ytrain[0: len(OOF_Preds)];\n\n        for fold_nb, (train_idx, dev_idx) in enumerate(cv.split(X,y_grp[0: len(OOF_Preds)])):\n            Xtr, Xdev = X.iloc[train_idx], X.iloc[dev_idx];\n            ytr, ydev = y.loc[Xtr.index],  y.loc[Xdev.index];\n            scores.append(ScoreMetric(ydev, np.average(Xdev, axis=1, weights = weights)));\n\n        collect();\n        clear_output();\n        return np.mean(scores);\n    \nclear_output();\nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:54:43.565515Z","iopub.execute_input":"2023-11-18T11:54:43.56607Z","iopub.status.idle":"2023-11-18T11:54:43.718821Z","shell.execute_reply.started":"2023-11-18T11:54:43.56604Z","shell.execute_reply":"2023-11-18T11:54:43.717738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nif CFG.ensemble_req == \"Y\" and CFG.optuna_req == \"Y\":   \n    PrintColor(f\"\\n{'-' * 20} Creating an Optuna Ensemble {'-' * 20}\\n\", \n               color = Fore.MAGENTA);   \n    \n    study = optuna.create_study(direction  = CFG.metric_obj, \n                                study_name = \"OptunaEnsemble\", \n                                sampler    = TPESampler(seed = CFG.state)\n                               );\n    study.optimize(Objective, \n                   n_trials          = CFG.ntrials, \n                   gc_after_trial    = True,\n                   show_progress_bar = True\n                  );\n    weights       = study.best_params;\n    clear_output();\n    \n    PrintColor(f\"\\n--> Post ensemble weights\\n\");\n    pprint(weights, indent = 5, width = 10, depth = 1);\n    PrintColor(f\"\\n--> Best ensemble CV score = {study.best_value :.5f}\\n\");\n    \n    # Making weighted predictions on the test set:-\n    sub_fl[CFG.target] = np.average(Mdl_Preds[methods], weights = list(weights.values()),\n                                    axis=1\n                                   ) / CFG.tgt_scl_fct;\n    \n    # Making OOF predictions:-\n    OOF_Preds[\"Optuna\"] = np.average(OOF_Preds[methods], weights = list(weights.values()),\n                                     axis=1\n                                    ) / CFG.tgt_scl_fct;\n    \n    PrintColor(f\"\\n--> Post ensemble test-set predictions\\n\");\n    display(sub_fl.head(5).style.format(precision = 5)); \n      \ncollect();\nprint();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:54:43.723142Z","iopub.execute_input":"2023-11-18T11:54:43.723503Z","iopub.status.idle":"2023-11-18T11:54:43.875909Z","shell.execute_reply.started":"2023-11-18T11:54:43.723472Z","shell.execute_reply":"2023-11-18T11:54:43.874764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nif CFG.ensemble_req == \"Y\" and CFG.LAD_req == \"Y\":\n    PrintColor(f\"\\n{'-' * 20} Creating an LADR Ensemble {'-' * 20}\\n\", color = Fore.MAGENTA); \n    X,y        = OOF_Preds[methods], ytrain[0: len(OOF_Preds)];\n    model      = LADR();\n    test_preds = 0;\n    scores     = [];\n    cv         = all_cv[CFG.enscv_mthd];\n    ens_preds  = pd.DataFrame(columns = [\"LAD\"]);\n\n    for fold_nb, (train_idx, dev_idx) in enumerate(cv.split(X, y_grp[0:len(X)])):\n        Xtr  = X.iloc[train_idx];  \n        Xdev = X.iloc[dev_idx]; \n        ytr  = y.loc[y.index.isin(Xtr.index)];\n        ydev = y.loc[y.index.isin(Xdev.index)];\n\n        model.fit(Xtr, ytr);\n        dev_preds  = PostProcessPred(model.predict(Xdev), CFG.pstprcs_train);\n        score      = ScoreMetric(ydev, dev_preds);\n        test_preds = test_preds + \\\n        PostProcessPred(model.predict(Mdl_Preds[methods]), CFG.pstprcs_train);\n        scores.append(score);\n        \n        ens_preds = pd.concat([ens_preds, \n                               pd.DataFrame(dev_preds, columns = ['LAD'], index = Xdev.index)], \n                              axis=0\n                             );\n\n    test_preds         = test_preds/ (CFG.n_splits * CFG.n_repeats);\n    sub_fl[CFG.target] = test_preds/ CFG.tgt_scl_fct;\n    ens_preds          = ens_preds.groupby(level = 0).mean().values;\n    OOF_Preds[\"LAD\"]   = ens_preds;\n    \n    PrintColor(f\"---> OOF scores for the LAD ensemble\");\n    with np.printoptions(linewidth = 160, precision= 4):\n        pprint(np.array(scores), indent = 10, depth = 1);\n        PrintColor(f\"---> Mean OOF scores for the LAD ensemble = {np.mean(scores):.5f}\");\n        \n    PrintColor(f\"\\n--> Post ensemble test-set predictions\\n\");\n    display(sub_fl.head(5).style.format(precision = 5)); \n    \n    # Analyzing the grouped predictions for edge case issues:-  \n    PrintColor(f\"\\n--> Analysis of grouped predictions on OOF data\\n\");\n    OOF_Preds['Grp'] = y_grp[0: len(OOF_Preds)];\n    OOF_Preds        = pd.concat([OOF_Preds, ytrain[0: len(OOF_Preds)]], axis=1);\n    \n    display(OOF_Preds[[\"Grp\", CFG.target, \"LAD\"]].\\\n            groupby(\"Grp\").\\\n            apply(lambda x: ScoreMetric(x[CFG.target], x[\"LAD\"])).\\\n            to_frame().rename(columns = {0: \"OOFScore\"}).\\\n            transpose().\\\n            style.format(precision = 4).\\\n            background_gradient(\"icefire\")\n           );\n\nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:54:43.877432Z","iopub.execute_input":"2023-11-18T11:54:43.877791Z","iopub.status.idle":"2023-11-18T11:54:45.948523Z","shell.execute_reply.started":"2023-11-18T11:54:43.87776Z","shell.execute_reply":"2023-11-18T11:54:45.94728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nif CFG.ensemble_req == \"Y\":\n    sub_fl[['id', CFG.target]].to_csv(f\"SubNoPP_V{CFG.version_nb}.csv\", index = None);\n    \n    PrintColor(f\"\\n---> Blending with good public work\\n\");\n    sub_fl['nb1'] = \\\n    pd.read_csv(f\"/kaggle/input/mohs-hardness-eda-transformation-xgboost-ann/submission.csv\")[CFG.target].values;\n    sub_fl['nb2'] = \\\n    pd.read_csv(f\"/kaggle/input/ps3-25-reg-tf-keras/submission.csv\")[CFG.target].values;   \n    \n    sub_fl[CFG.target] = PostProcessPred(sub_fl[CFG.target] * 0.025 + sub_fl['nb1'] * 0.90 + sub_fl['nb2'] * 0.075, \n                                         \"Y\", 1);\n       \n    PrintColor(f\"\\n---> Submission file- final check after post-processing\\n\");\n    display(sub_fl[CFG.target].describe());\n    sub_fl[['id', CFG.target]].to_csv(f\"Submission_V{CFG.version_nb}.csv\", index = None);\n    \n    PrintColor(f\"\\n---> Submission file after post-processing\\n\");\n    display(sub_fl[['id', CFG.target]].head(10).style.format(precision = 3));\n    \nif CFG.ML == \"Y\":  \n    OOF_Preds.add_prefix(f\"V{CFG.version_nb}_\").to_csv(f\"OOF_Preds_V{CFG.version_nb}.csv\");\n    Mdl_Preds.add_prefix(f\"V{CFG.version_nb}_\").to_csv(f\"Mdl_Preds_V{CFG.version_nb}.csv\"); \n    if isinstance(Scores, pd.DataFrame) == True:\n        Scores.to_csv(f\"Scores_V{CFG.version_nb}.csv\");\n          \ncollect();\nprint();","metadata":{"execution":{"iopub.status.busy":"2023-11-18T11:54:45.949999Z","iopub.execute_input":"2023-11-18T11:54:45.950757Z","iopub.status.idle":"2023-11-18T11:54:46.395677Z","shell.execute_reply.started":"2023-11-18T11:54:45.950714Z","shell.execute_reply":"2023-11-18T11:54:46.394716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #0052cc; border-bottom: 8px solid #cc9966\" > NEXT STEPS<br> <div> ","metadata":{}},{"cell_type":"markdown","source":"<div style= \"font-family: Cambria; letter-spacing: 0px; color:#000000; font-size:110%; text-align:left;padding:3.0px; background: #f2f2f2\" >\n1. We will perform a detailed EDA and elicit feature interactions and other relevant insights<br>\n2. Model tuning<br>\n3. Including other models in the ensemble <br>\n4. Better ensemble strategy <br>\n5. Any other discussion/ public work based insights <br>\n6. Significant improvement on edge predictions <br>\n7. <b>Is this a regression challenge? </b>\n</div>","metadata":{}}]}